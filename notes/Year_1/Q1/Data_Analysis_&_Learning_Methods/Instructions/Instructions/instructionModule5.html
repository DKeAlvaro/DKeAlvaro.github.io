<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Instructionmodule5 - MSc AI & Engineering Systems Notes">
    <title>Instructionmodule5 - MSc AIES</title>
    <link rel="icon" href="../../../../../../assets/svg/favicon.svg" type="image/svg+xml">
    <link rel="stylesheet" href="../../../../../../styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Initialize KaTeX
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
            
            // Handle theme switching for Prism.js
            function updatePrismTheme() {
                const isDark = document.body.classList.contains('dark-theme');
                const prismLink = document.querySelector('link[href*="prism"]');
                if (prismLink) {
                    if (isDark) {
                        prismLink.href = 'https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css';
                    } else {
                        prismLink.href = 'https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css';
                    }
                }
            }
            
            // Check initial theme
            updatePrismTheme();
            
            // Listen for theme changes
            const observer = new MutationObserver(function(mutations) {
                mutations.forEach(function(mutation) {
                    if (mutation.type === 'attributes' && mutation.attributeName === 'class') {
                        updatePrismTheme();
                    }
                });
            });
            
            observer.observe(document.body, {
                attributes: true,
                attributeFilter: ['class']
            });
        });
    </script>
    <style>
        .note-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
        }
        .note-header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 2px solid var(--border-color);
        }
        .note-title {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            color: var(--primary-color);
        }
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.4rem 0.8rem;
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.9rem;
            font-weight: 400;
            background: transparent;
            border: 1px solid var(--border-color);
            border-radius: 4px;
            transition: all 0.2s ease;
        }
        .back-link:hover {
            color: var(--primary-color);
            border-color: var(--primary-color);
        }
        .nav-left {
            display: flex;
            flex-direction: column;
            gap: 0.75rem;
        }
        
        .breadcrumb-section {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }
        .folder-path {
             font-size: 0.8rem;
             color: var(--text-color-secondary);
             opacity: 0.7;
         }
         .current-page-title {
             font-size: 1.3rem;
             font-weight: 700;
             color: var(--text-color);
             margin: 0;
             line-height: 1.3;
             letter-spacing: -0.01em;
         }
        .note-content {
            line-height: 1.8;
            color: var(--text-color);
        }
        .note-content h1, .note-content h2, .note-content h3 {
            color: var(--primary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }
        .note-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
            display: block;
        }
        /* Preserve centered images from markdown HTML */
        .note-content div[style*="text-align: center"] {
            text-align: center !important;
        }
        .note-content div[style*="text-align: center"] img {
            margin: 1rem auto;
        }
        .note-content pre {
            background: var(--code-bg);
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        .note-content code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .note-content blockquote {
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
            margin: 1rem 0;
            font-style: italic;
            opacity: 0.9;
        }
        .note-content ul, .note-content ol {
            padding-left: 2rem;
            margin: 1rem 0;
        }
        .note-content ul {
            list-style-type: disc;
        }
        .note-content li {
            margin-bottom: 0.2rem;
            display: list-item;
            list-style-position: outside;
        }
        /* Reduce spacing for nested lists */
        .note-content li ul, .note-content li ol {
            margin: 0.3rem 0;
            padding-left: 1.5rem;
        }
        .note-content ul li::marker {
            color: var(--primary-color);
            font-size: 1.2em;
        }
        
        /* Q&A Styles */
        .qa-item {
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
            background: var(--bg-color);
        }
        
        .qa-question {
            padding: 1rem;
            background: var(--secondary-bg, #f8f9fa);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 0.75rem;
            transition: background-color 0.2s ease;
            user-select: none;
        }
        
        .qa-question:hover {
            background: var(--hover-bg, #e9ecef);
        }
        
        .qa-icon {
            font-size: 0.8rem;
            transition: transform 0.2s ease;
            color: var(--primary-color);
            font-weight: bold;
        }
        
        .qa-question.active .qa-icon {
            transform: rotate(90deg);
        }
        
        .qa-text {
            font-weight: 600;
            color: var(--text-color);
            flex: 1;
        }
        
        .qa-answer {
            padding: 1rem;
            background: var(--bg-color);
            border-top: 1px solid var(--border-color);
            display: none;
            color: var(--text-color);
            line-height: 1.6;
        }
        
        .qa-answer.active {
            display: block;
        }
        
        /* Mobile responsive styles */
        @media (max-width: 768px) {
            .note-container {
                padding: 0.5rem;
            }
            .navigation-header {
                padding: 1rem;
                gap: 1rem;
                margin-bottom: 1.5rem;
            }
            .nav-left {
                gap: 1rem;
            }
            .breadcrumb-section {
                flex-direction: column;
                align-items: flex-start;
                gap: 0.5rem;
            }
            .current-page-title {
                font-size: 1.1rem;
                text-align: left;
            }
            .back-link {
                font-size: 0.9rem;
                padding: 0.6rem 1rem;
                min-height: 40px;
                display: flex;
                align-items: center;
                justify-content: center;
                border-radius: 6px;
                background: var(--border-color);
                color: var(--text-color) !important;
                text-decoration: none;
                border: 1px solid var(--border-color);
                transition: all 0.2s ease;
                flex-shrink: 0;
            }
            .back-link:hover {
                background: var(--primary-color);
                color: white !important;
                border-color: var(--primary-color);
            }
            .folder-path {
                font-size: 0.85rem;
                color: var(--text-color-secondary);
                opacity: 0.8;
                flex: 1;
                min-width: 0;
            }
            .note-navigation {
                width: 100%;
                display: flex;
                align-items: center;
                gap: 0.75rem;
                flex-wrap: nowrap;
            }
            .nav-button {
                font-size: 1.2rem;
                padding: 0.5rem;
                min-height: 40px;
                min-width: 40px;
                border-radius: 50%;
                display: flex;
                align-items: center;
                justify-content: center;
                flex-shrink: 0;
            }
            .nav-button span:not(.nav-icon) {
                display: none;
            }
            .nav-progress {
                flex: 1;
                font-size: 0.8rem;
                text-align: center;
                color: var(--text-color-secondary);
            }
            .note-header {
                margin-bottom: 1rem;
                padding-bottom: 1.5rem;
            }
            .note-title {
                font-size: 2rem;
            }
            .note-content h1 {
                font-size: 1.8rem;
            }
            .note-content h2 {
                font-size: 1.5rem;
            }
            .note-content h3 {
                font-size: 1.3rem;
            }
            .note-content pre {
                padding: 0.75rem;
                font-size: 0.9rem;
            }
            .note-content ul, .note-content ol {
                padding-left: 1.5rem;
            }
        }
        
        @media (max-width: 480px) {
             .note-container {
                 padding: 0.5rem;
             }
             .note-header {
                 margin-bottom: 1.15rem;
                 padding-bottom: 0.75rem;
             }
             .back-link {
                 padding: 0.25rem 0.5rem;
                 font-size: 0.85rem;
             }
         }
        
        /* Navigation header styles */
         .navigation-header {
             display: flex;
             flex-direction: column;
             gap: 1.25rem;
             margin-bottom: 2rem;
             padding: 1.25rem;
             background: transparent;
             border-radius: 0;
             border: none;
             border-bottom: 1px solid var(--border-color);
         }
         
         /* Navigation buttons styles */
         .note-navigation {
             display: flex;
             justify-content: center;
             align-items: center;
             gap: 1rem;
         }
         
         .nav-button {
             display: inline-flex;
             align-items: center;
             justify-content: center;
             padding: 0.6rem;
             background: transparent;
             border: 1px solid var(--border-color);
             border-radius: 4px;
             color: var(--text-color);
             text-decoration: none;
             font-size: 1rem;
             font-weight: 400;
             transition: all 0.2s ease;
             width: 36px;
             height: 36px;
         }
         
         .nav-button:hover {
             color: var(--primary-color);
             border-color: var(--primary-color);
         }
         
         .nav-button:disabled {
             opacity: 0.5;
             cursor: not-allowed;
             pointer-events: none;
         }
         
         .nav-button.prev {
             justify-content: flex-start;
         }
         
         .nav-button.next {
             justify-content: flex-end;
         }
         
         .nav-button-text {
             display: flex;
             flex-direction: column;
             align-items: inherit;
         }
         
         .nav-button-label {
             font-size: 0.75rem;
             opacity: 0.7;
             margin-bottom: 0.2rem;
         }
         
         .nav-button-title {
             font-weight: 500;
             max-width: 200px;
             overflow: hidden;
             text-overflow: ellipsis;
             white-space: nowrap;
         }
         
         .nav-progress {
             display: flex;
             flex-direction: column;
             align-items: center;
             gap: 0.5rem;
             flex: 1;
             margin: 0 1rem;
         }
         
         .nav-progress-text {
             font-size: 0.8rem;
             font-weight: 400;
             color: var(--text-color);
             opacity: 0.7;
             text-align: center;
         }
         
         .nav-progress-bar {
             width: 100%;
             max-width: 200px;
             height: 4px;
             background: #f0f0f0;
             border-radius: 2px;
             overflow: hidden;
         }
         
         .nav-progress-fill {
             height: 100%;
             background: #007acc;
             transition: width 0.3s ease;
         }
         
         @media (max-width: 768px) {
              .note-navigation {
                  flex-direction: row;
                  gap: 0.75rem;
                  align-items: center;
              }
              
              .nav-button {
                  width: 40px;
                  min-width: 40px;
                  height: 40px;
                  border-radius: 50%;
                  padding: 0.5rem;
                  font-size: 1.2rem;
              }
              
              .nav-progress {
                  flex: 1;
                  margin: 0;
              }
              
              .nav-button-title {
                  max-width: none;
              }
          }
          
          /* Jupyter Notebook Styles */
          .code-cell {
              margin: 1.5rem 0;
              border: 1px solid var(--border-color);
              border-radius: 8px;
              overflow: hidden;
              background: var(--bg-color);
          }
          
          .markdown-cell {
              margin: 1.5rem 0;
              padding: 1rem;
              background: var(--bg-color);
              border-radius: 8px;
          }
          
          .input-area {
              background: var(--code-bg, #f8f9fa);
              border-bottom: 1px solid var(--border-color);
          }
          
          .input-area pre {
              margin: 0;
              padding: 1rem;
              background: transparent;
              border-radius: 0;
              overflow-x: auto;
          }
          
          .input-area code {
              background: transparent;
              padding: 0;
              border-radius: 0;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.9rem;
              line-height: 1.4;
          }
          
          .output-area {
              background: var(--bg-color);
              padding: 1rem;
          }
          
          .output-stream {
              background: var(--secondary-bg, #f1f3f4);
              padding: 0.75rem;
              margin: 0.5rem 0;
              border-radius: 4px;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.85rem;
              line-height: 1.4;
              color: var(--text-color);
              border-left: 3px solid var(--primary-color);
          }
          
          .output-result {
              background: var(--bg-color);
              padding: 0.75rem;
              margin: 0.5rem 0;
              border-radius: 4px;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.85rem;
              line-height: 1.4;
              color: var(--text-color);
              border: 1px solid var(--border-color);
          }
          
          .output-image {
              text-align: center;
              margin: 1rem 0;
              padding: 0.5rem;
              background: var(--bg-color);
              border-radius: 4px;
          }
          
          .output-image img {
              max-width: 100%;
              height: auto;
              border-radius: 4px;
              box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
              margin: 0;
          }
          
          .output-html {
              margin: 0.5rem 0;
              padding: 0.75rem;
              background: var(--bg-color);
              border-radius: 4px;
              border: 1px solid var(--border-color);
              overflow-x: auto;
          }
          
          .output-html table {
              width: 100%;
              border-collapse: collapse;
              margin: 0;
          }
          
          .output-html th,
          .output-html td {
              padding: 0.5rem;
              border: 1px solid var(--border-color);
              text-align: left;
          }
          
          .output-html th {
              background: var(--secondary-bg, #f8f9fa);
              font-weight: 600;
          }
          
          .output-error {
              background: #fef2f2;
              border: 1px solid #fecaca;
              border-radius: 4px;
              margin: 0.5rem 0;
              padding: 0.75rem;
          }
          
          .error-name {
              color: #dc2626;
              font-weight: 600;
              margin: 0 0 0.5rem 0;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.9rem;
          }
          
          .error-traceback {
              color: #7f1d1d;
              margin: 0;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.8rem;
              line-height: 1.4;
              background: #fef7f7;
              padding: 0.5rem;
              border-radius: 3px;
              overflow-x: auto;
          }
          
          /* Dark theme adjustments for error outputs */
          .dark-theme .output-error {
              background: #2d1b1b;
              border-color: #7f1d1d;
          }
          
          .dark-theme .error-name {
              color: #f87171;
          }
          
          .dark-theme .error-traceback {
              color: #fca5a5;
              background: #1f1515;
          }
          
          /* Dark mode adjustments for Jupyter cells */
          @media (prefers-color-scheme: dark) {
              .input-area {
                  background: var(--code-bg, #2d3748);
              }
              
              .output-stream {
                  background: var(--secondary-bg, #4a5568);
              }
          }
    </style>
</head>
<body>
    <div class="menu-overlay"></div>
    <header>
        <nav class="nav-container">
            <div class="hamburger" id="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
            <ul class="nav-links" id="navLinks">
                <li><a href="../../../../../../index.html">About</a></li>
                <li><a href="../../../../../../journey.html">My Life</a></li>
                <li><a href="../../../../../../projects.html">Projects</a></li>
                <li><a href="../../../../../../blogs.html">Blog</a></li>
                <li><a href="../../../../../../notes.html">MSc AIES</a></li>
                <li><a href="https://dailyclips.es/" target="_blank">Daily Clips</a></li>
                <li><a href="https://alvaromenendez.es/ufc-predictions/" target="_blank">UFC Predictions</a></li>
                <li><a href="../../../../../../acknowledgments.html">Acknowledgments</a></li>
                <li><a href="../../../../../../assets/Alvaro_Menendez_CV.pdf" download>CV</a></li>
            </ul>
            <div class="nav-right">
                <div class="social-links">
                    <a href="https://github.com/DKeAlvaro" target="_blank" aria-label="GitHub">
                        <img src="../../../../../../assets/svg/github.svg" alt="GitHub" class="social-icon">
                    </a>
                    <a href="https://www.linkedin.com/in/alvaromenendezros" target="_blank" aria-label="LinkedIn">
                        <img src="../../../../../../assets/svg/linkedin.svg" alt="LinkedIn" class="social-icon">
                    </a>
                    <a href="mailto:alvaro.mrgr@gmail.com" aria-label="Email">
                        <img src="../../../../../../assets/svg/gmail.svg" alt="Email" class="social-icon">
                    </a>
                </div>
                <div class="theme-toggle" id="themeToggle" aria-label="Toggle theme" role="button" tabindex="0">
                    <img src="../../../../../../assets/svg/moon.svg" alt="Toggle theme" class="theme-icon" id="themeIcon">
                </div>
            </div>
        </nav>
    </header>

    <div class="page-container">
        <div class="content">
            <div class="note-container">
                <div class="navigation-header">
                    <div class="nav-left">
                        <div class="breadcrumb-section">
                            <a href="../../../../../../notes.html" class="back-link">
                                <span>←</span>
                                <span>Back to Notes</span>
                            </a>
                            <span class="folder-path">Year_1 / Q1 / Data_Analysis_&_Learning_Methods / Instructions</span>
                        </div>
                        <div class="current-page-title" id="currentPageTitle"></div>
                    </div>
                    
                    <div class="note-navigation" id="noteNavigation">
                        <a href="#" class="nav-button prev" id="prevButton" style="visibility: hidden;" title="Previous note">
                            <span class="nav-icon">←</span>
                        </a>
                        
                        <div class="nav-progress">
                            <div class="nav-progress-text" id="progressText">Loading...</div>
                            <div class="nav-progress-bar">
                                <div class="nav-progress-fill" id="progressFill" style="width: 0%;"></div>
                            </div>
                        </div>
                        
                        <a href="#" class="nav-button next" id="nextButton" style="visibility: hidden;" title="Next note">
                            <span class="nav-icon">→</span>
                        </a>
                    </div>
                </div>
                
                <div class="note-content">
                    <div class="markdown-cell">
<h1>Instruction Session 5</h1>
<p>In this session, we will explore and compare different clustering techniques.<br />
The focus will be on understanding their assumptions, strengths, and limitations.  </p>
<p>We will cover the following algorithms:</p>
<ol>
<li><strong>K-Means</strong> – a centroid-based algorithm that partitions data into clusters of roughly spherical shape.  </li>
<li><strong>Gaussian Mixture Models (GMMs)</strong> – a probabilistic approach that assumes data is generated from a mixture of Gaussian distributions.  </li>
<li><strong>Self-Organizing Maps (SOMs)</strong> – a neural network–based technique that projects high-dimensional data into a lower-dimensional (typically 2D) grid.  </li>
<li><strong>DBSCAN</strong> – a density-based algorithm that can discover arbitrarily shaped clusters and identify noise/outliers.</li>
</ol>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# this cell imports the libraries or packages that you can use during this assignment
# you are not allowed to import additional libraries or packages

from helpers import *
import os
import itertools
import numpy as np
import pandas as pd

# Machine Learning and Decomposition
from sklearn.decomposition import PCA, FastICA
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
import skfuzzy as fuzz

# Statistical tools
from scipy.stats import multivariate_normal
from scipy import linalg

# Visualization
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from matplotlib.colors import ListedColormap, LogNorm
import seaborn as sns
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>Part 1: K-means algorithm</h2>
<p>In this part we will discuss unsupervised machine learning problems and describe how the K-means algorithm can be used to solve these.</p>
<p>Unsupervised machine learning problems are problems in which we try to determine some particular structure within a data set. On the contrary, supervised machine learning problems require us to model some kind of input-output mapping. Unsupervised machine learning problems do not have a specified output which we would like to model. Instead we are interesting in making sense of the data in grouping this data, without knowing beforehand which and how many groups exist.</p>
<p>The K-means algorithm can group $N$ data samples of dimension $D$ into $K$ groups or clusters. These clusters can each be characterized by their mean vector: the expected or average value of the points which are assigned to the cluster. The mean vector denoting the center of the $k^{th}$ cluster can be represented as the column vector ${\bf{\mu}}^{(k)} = [\mu_1^{(k)},\ \mu_2^{(k)},\ \ldots, \mu_D^{(k)}]^\top$ and the $n^{th}$ data sample can be represented by the column vector ${\bf{x}}^{(n)} = [x_1^{(n)},\ x_2^{(n)},\ \ldots, x_D^{(n)}]^\top$, where the superscript denotes the sample index.</p>
<p>The K-means algorithm tries to minimize the (within-cluster) Euclidean squared distance
$$J({\bf{X}}, {\bf{\mu}}) = \frac{1}{N}\sum_{n=1}^N \sum_{k=1}^K \rho_k^{(n)} | {\bf{x}}^{(n)} - {\bf{\mu}}^{(k)}|^2$$
Here $\rho_k^{(n)}$ is a so-called indicator function that is defined as 
$$ \rho_k^{(n)} = \begin{cases} 1 &amp; \text{if sample }{\bf{x}}^{(n)}\text{ is assigned to cluster }k \ 0 &amp; \text{otherwise}\end{cases}$$
This indicator function equals $1$ when the corresponding data point is assigned to the corresponding cluster and $0$ otherwise. The cost function therefore represents the average squared distance with respect to the cluster that a point is assigned to.</p>
<p>The algorithm is specified as follows:</p>
<ol>
<li>Initialize means ${\bf{\mu}}$.</li>
<li>Assign data points to closest cluster mean (i.e. update $\rho_k^{(n)}$).</li>
<li>Calculate new cluster means as the average values of the points that are assigned to it (i.e. update ${\bf{\mu}}$).</li>
<li>Calculate cost function $J({\bf{X}}, {\bf{\mu}})$.</li>
<li>If not converged, go back to 2 and repeat.</li>
</ol>
<p>Here we will describe the algorithm in words. First the centers of the clusters are initialized. This can be done arbitrarily, but often the centers are set to random (but distinct) samples of the data set.
Once the means are set, we assign each data sample to the cluster that is closest to it. In order to do so, we calculate the Euclidean squared distance between a point and all the clusters and find the cluster that is closest to it. We repeat this for all points and we therefore completely specify $\rho_k^{(n)}$. Once all points have been assigned to a cluster, we look up all points corresponding to a certain cluster and we average these to calculate the new cluster center. We update all cluster means. Then we evaluate the current fit of the clusters on the data by evaluate the cost function. If we still see a significant improvement in the cost function, we repeat updating the assignments and cluster centers and if the cost function seems to have converged, we stop iterating.</p>
<p>In this part of the assignment you will implement the K-means algorithm from scratch, starting with the initialization of the cluster means.</p>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 1.1: Initializing cluster centers</h3>
<p>Consider the function <code>X = ex3_generate_data()</code> which generates a matrix ${\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function <code>means = initialize_means(X, K)</code> that accepts the data set ${\bf{X}}$  and number of clusters $K$ as input and returns a matrix of shape (K x D), representing the vertical concatenation of $K$ transposed mean vectors of dimension $D$. These means should be initialized such that they coincide with <em>random</em> samples from the data set, which are always <em>distinct</em>. In other words, the means should equal a random subset of the availabe data set, where no means are equal. Also keep in mind that the number of clusters is variable in this function.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the function initialize_means(X, K)

def initialize_means(X, K):

    # get number of samples 
    N = np.shape(X)[0]

    # fetch random indices of data
    inds = np.arange(N)
    np.random.shuffle(inds)
    inds = inds[:K]

    # fetch means
    means = X[inds,:]

    # return means
    return means
    
#// END_TODO
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# generate data
X = ex3_generate_data()

# initialize means
means = initialize_means(X, 3)

# plot data
plt.figure()
plt.scatter(X[:,0], X[:,1], 10)
plt.scatter(means[:,0], means[:,1], c="red", marker="x")
plt.grid()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 1.1</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<p>Now that the clusters have been initialized, it is time to assign points to the closest clusters.</p>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 1.2: Assign points to clusters</h3>
<p>Again consider the function <code>X = ex4_generate_data()</code> which generates a matrix ${\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function <code>rho = assign_data_to_clusters(X, means)</code> that accepts the data set ${\bf{X}}$ and matrix of means ${\bf{\mu}}$ as input and returns a matrix of shape (N x K), which contains all indicator functions $\rho_k^{(n)}$. This matrix should be a matrix of only ones and zeros and each row should sum to 1.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the function assign_data_to_clusters(X, means)

def assign_data_to_clusters(X, means):

    # fetch dimensions 
    N = np.shape(X)[0]
    K = np.shape(means)[0]

    # allocate array of rho
    rho = np.zeros((N, K))

    # loop through samples
    for n in range(N):

        # allocate array for distances
        J = np.zeros(K)

        # loop through clusters
        for k in range(K):

            # calculate distance
            J[k] = np.linalg.norm(X[n,:] - means[k,:])**2

        # select closest clusters and put rho to 1
        rho[n, np.argmin(J)] = 1

    # return rho
    return rho

#// END_TODO 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# generate data
X = ex3_generate_data()

# initialize means
means = initialize_means(X, 3)

# assign point to clusters
rho = assign_data_to_clusters(X, means)

# plot data
plt.figure()
plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))
plt.scatter(means[:,0], means[:,1], c="red", marker="x")
plt.grid()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 1.2</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<p>The means have been initialized, the point have been assigned to a cluster. Now the cluster centers can be updated.</p>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 1.3: Update cluster centers</h3>
</div>
<div class="markdown-cell">
<p>Again consider the function <code>X = ex3_generate_data()</code> which generates a matrix ${\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ data vectors of dimension $D$. Create a function <code>means = update_cluster_centers(X, rho)</code> that accepts the data set ${\bf{X}}$ and matrix of indicators $\rho$ as input and returns a matrix of shape (K x D), which contains the new cluster centers.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO  Complete the function update_cluster_centers(X, rho)
def update_cluster_centers(X, rho):

    # fetch dimensions 
    (N, K) = np.shape(rho)
    D = np.shape(X)[1]

    # allocate new means
    means = np.zeros((K,D))

    # loop through clusters
    for k in range(K):

        # update means
        means[k,:] = np.mean(X[np.argmax(rho,axis=1)==k,:], axis=0)


    # return means
    return means

#// END_TODO 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# generate data
X = ex3_generate_data()

# initialize means
means = initialize_means(X, 3)

# assign point to clusters
rho = assign_data_to_clusters(X, means)

# update means
means_new = update_cluster_centers(X, rho)

# plot data
plt.figure()
plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))
plt.scatter(means[:,0], means[:,1], 50, c="red", marker="x")
plt.scatter(means_new[:,0], means_new[:,1], 50, c="blue", marker="x")
plt.grid()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 1.3</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<p>Almost there! Now it is just a matter of combining the previous functions for finalizing the K-means algorithms.</p>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 1.4: Implement K-means algorithm</h3>
<p>First create a function <code>J = Kmeans_loss(X, means, rho)</code> that calculates the within-cluster Euclidean squared distance as defined above. Secondly create the final <code>means, rho, J = Kmeans(X, K)</code> function that combines all previous functions to create the K-means algorithm as specified in the the introduction of this part of the assignment. This function returns the final cluster centers, the indicator function and a history of the losses. Save the loss <em>after</em> each iteration and stop iterating when the difference in loss does no longer exceed 1e-10. The initial loss based on the randomly initialized means should not be returned.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the Kmeans_loss(X, means, rho) function

def Kmeans_loss(X, means, rho):

    # get dimensions
    (K, D) = np.shape(rho)
    N = np.shape(X)[0]

    # initialize loss function
    J = 0

    # loop through samples
    for n in range(N):

        J += 1/N*np.linalg.norm(X[n,:] - means[np.argmax(rho, axis=1)[n],:])**2

    # return loss 
    return J
    
#// END_TODO 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO  Complete the Kmeans(X, K) function
def Kmeans(X, K):

    # initialize means
    means = initialize_means(X, K)

    # perform assignment
    rho = assign_data_to_clusters(X, means)

    # initialize loss
    J = np.append([1e10], Kmeans_loss(X, means, rho))

    # iterate until convergence
    while np.abs(J[-1] - J[-2]) > 1e-10:

        # update means
        means = update_cluster_centers(X, rho)

        # perform assignment
        rho = assign_data_to_clusters(X, means)

        # calculate loss
        J = np.append(J, Kmeans_loss(X, means, rho))

    # return parameters
    return means, rho, J[1:]

#// END_TODO 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# generate data
X = ex3_generate_data()

# initialize means
means, rho, J = Kmeans(X, 3)

# plot data
_,ax = plt.subplots(ncols=2, figsize=(15,5))
ax[0].scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))
ax[0].scatter(means[:,0], means[:,1], 50, c="red", marker="x")
ax[1].plot(J)
ax[0].grid(), ax[1].grid(), ax[1].set_ylabel("cost function"), ax[1].set_xlabel("iteration");
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 1.4</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 1.5: Number of clusters</h3>
<p>In the previous assignment the data had been generate from 3 clusters. In practice the number of clusters is often unknown. In this exercise we will see what happens when we add extra clusters. In this exercise, run your K-means algorithm on the previous data set for 2 up to and including 10 clusters and save the final value of the loss (i.e. the loss value when the algorithm has converged).</p>
<p>Plot the final loss against the number of used clusters.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# generate data
X = ex3_generate_data()
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Plot the Kmeans loss against the number of clusters

# perform K means
J_mem = np.zeros(9)
for k in range(2,11):
    _, _, J = Kmeans(X, k)
    J_mem[k-2] = J[-1]

# plot data
plt.figure()
plt.plot(range(2,11), J_mem)
plt.grid(), plt.ylabel("cost function"), plt.xlabel("number of clusters");

#// END_TODO 
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 1.5</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 1.6: Shortcoming of the K-means algorithm</h3>
<p>Apply the Kmeans algorithm for the new data set generate by <code>X = ex36_generate_data()</code>. Visualise the data and come up with an appropriate number of clusters. Plot the data points in a scatter plot, plot the means as red crosses in the same plot and color the data point according to their assigned cluster.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# generate data
X = ex36_generate_data()
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO  Plot clusters of new data set

# initialize means
means, rho, J = Kmeans(X, 3)

# visualize results
plt.figure()
plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))
plt.scatter(means[:,0], means[:,1], 50, c="red", marker="x")
plt.grid()

#// END_TODO 
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 1.6</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>Part 2: Gaussian Mixture modeling</h2>
<p>The K-Means algorithm is a fast and simple method that works well for many applications. However, due to its simplicity, it isn't suitable for all situations. Fuzzy C-Means (FCM) improves on K-Means by allowing for soft clustering, where data points can belong to multiple clusters with varying degrees of membership. While this flexibility is advantageous, FCM has its own set of limitations. It can be computationally intensive, especially for large datasets, and is sensitive to the initial selection of cluster centers. Additionally, FCM may struggle with outliers, varying cluster sizes, and high-dimensional data, which can lead to suboptimal clustering results.</p>
<p>In this part we present another methodology for clustering data, namely through Gaussian mixture modeling. In this approach we do not rely on a deterministic algorithm for determining the cluster means and assignments, but instead we model the data set by a probability density function.</p>
<p>We will assume that the data set has been generated from a Gaussian mixture model, which is formally specified as
$$ p({\bf{x}}^{(n)}) = \sum_{k=1}^K \rho_k \mathcal{N}({\bf{x}}^{(n)} \mid {\bf{\mu}}<em k="1">k, \Sigma_k),$$
where a data sample ${\bf{x}}^{(n)}$ is originating from a Gaussian mixture model with $K$ individual Gaussian distributions with means ${\bf{\mu}}_k$ and covariance matrices ${\bf{\Sigma}}_k$. The mean denotes the center or mode of the Gaussian distribution and the covariance matrix specifies the strech and tilt of the Gaussian distribution. In this model the mixing coefficients $\rho_k$ specify how much each of the Gaussian distributions contributes in the model. Because the Gaussian mixture model is a probability density function, integrating over ${\bf{x}}$ should always equal 1. Because the individual Gaussians already satisfy this requirement, the mixing coefficients are constrained by
$$ \sum</em>^K \rho_k = 1.$$
To give some intuition on this model, we give a 1-dimensional example below. Here we model a data set by a mixture of 2 Gaussians. The individual <em>weighted</em> Gaussian distributions are colored in blue and the corresponding mixture model distribution is colored in red.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
ex4_plot_GMM_1D()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p>During this part of the assignment you will implement the so-called Expectation-Maximization (EM) algorithm for learning the Gaussian mixture model. This algorithm consists of two step, the expectation step (E-step) and the maximization step (M-step). The exact details of the algorithm are beyond the scope of this assignment, but here we will present the update equations for these steps.</p>
<p>The EM algorithm works as follows:</p>
<ol>
<li>Initialize the means ${\bf{\mu}}_k$, covariances $\Sigma_k$ and mixing coefficients $\rho_k$. Often the means are initialized using the Kmeans algorithm. The covariance matrices can be set to identity matrices and the mixing coefficients can be initialized to the fraction of points assigned to the cluster with Kmeans divided by the total number of samples.</li>
<li><em>Expectation step</em>: evaluate the responsibilities $\gamma_{nk}$ using the current parameter values as 
$$ \gamma_{nk} = \frac{\rho_k \mathcal{N}({\bf{x}}<em j="1">n \mid {\bf{\mu}}_k, \Sigma_k)}{\sum</em>$$}^K \rho_j \mathcal{N}({\bf{x}}_n \mid {\bf{\mu}}_j, \Sigma_j)</li>
<li><em>Maximization step</em>: re-estimate the parameters using the current responsibilities
$$ {\bf{\mu}}<em n="1">k^\text{new} = \frac{1}{N_k} \sum</em>}^N \gamma_{nk}{\bf{x}<em n="1">n $$
$$ \Sigma_k^\text{new} = \frac{1}{N_k} \sum</em>}^N \gamma_{nk} ({\bf{x}<em n="1">n - {\bf{\mu}}_k^\text{new})({\bf{x}}_n - {\bf{\mu}}_k^\text{new})^\top $$
$$ \rho_k = \frac{N_k}{N} $$
where $N$ denotes the number of samples and where
$$ N_k = \sum</em>$$}^N \gamma_{nk</li>
<li>Evaluate the log-likelihood
$$ \ln p({\bf{X}} \mid {\bf{\mu}}, \Sigma, {\bf{\rho}}) = \sum_{n=1}^N \ln \left{ \sum_{k=1}^K \rho_k \mathcal{N}({\bf{x}}^{(n)} \mid {\bf{\mu}}_k, \Sigma_k)\right}$$</li>
</ol>
<p>It is important to grasp what is going on in this algorithm. The responsibilities $\gamma_{nk}$ are similar to the indicator functions from the Kmeans algorithm. However, where the Kmeans algorithm performs a hard clustering (each point can be assigned to only 1 cluster), the Gaussian mixture model allows for a soft clustering (each point can be modeled by both Gaussian distribution, but just to a different extent). The indicator function of the Kmeans algorithm was one-hot coded, meaning that a point was assigned to 1 cluster only. The responsibilities $\gamma_{nk}$ specify how likely a data sample ${\bf{x}}_n$ is to be generated from a cluster. With a Gaussian mixture model a point can therefore be assigned to different extents to multiple clusters. The expectation step calculates these responsibilities and the division in this expression makes sure that all rows sum op to 1.</p>
<p>In the maximization step the parameters are updated. Here the contribution of each data sample towards the parameters depends on the corresponding responsibilities. This means that a point that is very likely to have originated from a certain cluster will have a high influence on the statistics of that cluster. The variable $N_k$ specifies how many points are located to a certain Gaussian distribution. Because this parameter is the summation over the individual responsibilities, $N_k$ is not forced to be an integer.</p>
<p>The log-likelihood is a cost function which takes the variances and uncertainties in our model into account. It describes the probability of that data set being generated from a Gaussian mixture model. To prevent numerical instability we commonly use the log-likelihood instead of the normal likelihood.</p>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 2.1: Initialize clusters</h3>
<p>Consider the function from the previous part <code>X = ex46_generate_data()</code> which generates a matrix ${\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function <code>means, covs, rho = initialize_GMM(X, K)</code> that accepts the data set ${\bf{X}}$ as input and returns the following in this order:
- <code>means</code>: a matrix of size (K x D) that contains the initial cluster means, as a vertical concatenation of the transposed mean vectors. These means should be initialized using the previously written K-means algorithm.
- <code>covs</code>: a matrix of size (K x D x D) that contains the covariance matrices of the initial clusters. Each matrix <code>covs[k,:,:]</code> represents the covariance matrix of the $k^\text{th}$ cluster. Initialize these covariance matrices as identity matrices.
- <code>rho</code>: a vector of length K that contains the mixing coefficients as specified above. Initialize this vector based on the indicator function returned by the K-means algorithm.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the initialize_GMM(X, K) function


    
#// END_TODO
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# fetch data
X = ex36_generate_data()

# initialize GMM
means, covs, rho = initialize_GMM(X, 2)

# plot GMM
ex4_plot_GMM(X, means, covs, rho)
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 2.1</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 2.2: Expectation step</h3>
<p>Create a function <code>gamma = expectation_step(X, means, covs, rho)</code> that accepts the data set, means, covariances and mixing coefficients with dimensions specified above. This function should perform the expectation step and should return the calculated responsibilities as defined above as a matrix of size (N x K) where each row corresponds to the assignment fraction of a sample amongst the different clusters. Make sure this matrix is properly normalized such that the elements in each row add up to 1. Use the <code>multivariate_normal</code> function that has been imported from <code>scipy.stats</code> at the beginning of this notebook.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO  Complete the expectation_step(X, means, covs, rho) function


#// END_TODO
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
gamma = expectation_step(X, means, covs, rho)
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 2.2</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 2.3: Maximization step</h3>
<p>Create a function <code>means, covs, rho = maximization_step(X, gamma)</code> that accepts the data set and responsibilities with dimensions specified above. This function should perform the maximization step and should return the new means, covariances and mixing coefficients with dimensions as specified above.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the maximization_step(X, gamma) function



#// END_TODO 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# maximization step
means, covs, rho = maximization_step(X, gamma)

# plot GMM
ex4_plot_GMM(X, means, covs, rho)
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 2.3</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 2.4: Log-likelihood calculation</h3>
<p>Create a function <code>J = loglikelihood(X, means, covs, rho)</code> that accepts the data set, means, covariance matrices and mixing coefficients with dimensions specified above. This function should calculate and return the log-likelihood of the data under the specified Gaussian mixture model. Use the definition as specified in the beginning of Part 2.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the loglikelihood(X, means, covs, rho) function



#// END_TODO 
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 2.4</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 2.5: Gaussian mixture modeling</h3>
<p>Now that all the subfunctions have been defined it is time to tie them together and to form a function which does the Gaussian mixture modelling. Create a function <code>means, covs, rho, gamma, J = GMM_modeling(X, K, nr_iterations)</code> that does the following:</p>
<ol>
<li>Initialize the parameters of the Gaussian mixture model.</li>
<li>Performs <code>nr_iterations</code> iterations of the following:<ol>
<li>Perform the expectation step.</li>
<li>Perform the maximization step.</li>
<li>Calculate the log-likelihood.</li>
</ol>
</li>
<li>returns the parameters and a vector of saved values of the log-likelihood.</li>
</ol>
<p>The function should return all the parameters of the trained Gaussian mixture model, containing the final means, covariance matrices, mixing coefficients, responsibilities and a vector containing all calculated values of the log-likelihood.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
#// BEGIN_TODO Complete the GMM_modeling(X, K, nr_iterations) function



#// END_TODO 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# train GMM
means, covs, rho, gamma, J = GMM_modeling(X, 2, 10)

# plot GMM
ex4_plot_GMM(X, means, covs, rho)
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
plt.figure()
plt.plot(J)
plt.grid(), plt.xlabel("iteration"), plt.ylabel("log-likelihood");
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>End of exercise 2.5</h3>
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>Part 3: Kohonen Maps or Self Organizing Maps</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 3.1: Self Organizing maps for outlier detection</h3>
<p>In this task, we will capitalize the power of outlier detection to isolate the outliers from a toy dataset, run the following code to visualize the data.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
from minisom import MiniSom

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs
from sklearn.preprocessing import scale

outliers_percentage = 0.35
inliers = 300
outliers = int(inliers * outliers_percentage)


data = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[.3, .3],
                  n_samples=inliers, random_state=0)[0]

data = scale(data)
data = np.concatenate([data, 
                       (np.random.rand(outliers, 2)-.5)*4.])

plt.figure(figsize=(8, 8))
plt.scatter(data[:, 0], data[:, 1])
plt.show()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p>What we expect from a good outlier algorithm is that all the samples far away from the two main clusters are labeled as outliers. This can be obtained considering as outliers the samples with a high quantization error.</p>
<p><strong>To test this idea you have the code to 1) train a SOM, 2) compute the quantization error, 3) set a treshold for the quantization error</strong></p>
<p><strong>Can you adjust the threshold such that you isolate the outliers in the plot?</strong></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
som = MiniSom(2, 1, data.shape[1], sigma=1, learning_rate=0.5,
              neighborhood_function='triangle', random_seed=10)


som.train(data, 100, random_order=False, verbose=True)  # random training

# the Euclidean distance between each data point and its closest BMU (best matching unit) in the SOM
quantization_errors = np.linalg.norm(som.quantization(data) - data, axis=1)
# np.percentile(data, p) Returns the value below which p% of the data lies.
outliers_percentage = 0.10
error_treshold = np.percentile(quantization_errors, 
                               100*(1-outliers_percentage))

print('Error treshold:', error_treshold)

is_outlier = quantization_errors > error_treshold

plt.hist(quantization_errors)
plt.axvline(error_treshold, color='k', linestyle='--')
plt.xlabel('error')
plt.ylabel('frequency')
plt.figure(figsize=(8, 8))
plt.scatter(data[~is_outlier, 0], data[~is_outlier, 1],
            label='inlier')
plt.scatter(data[is_outlier, 0], data[is_outlier, 1],
            label='outlier')
plt.legend()
plt.show()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>Part 4: DBSCAN</h2>
</div>
<div class="markdown-cell">
<h2>---</h2>
<h3>Exercise 4.1: Comparing K-means and DBSCAN</h3>
<p>Run the following code and answer the questions:
1. <strong>Cluster Shapes:</strong><br />
   Looking at the DBSCAN and K-Means plots, which algorithm correctly identified the crescent-shaped clusters in the <code>make_moons</code> dataset? Explain why one algorithm succeeded while the other struggled.</p>
<ol>
<li><strong>Noise and Outliers:</strong><br />
   DBSCAN can label some points as noise. Are there any points in the DBSCAN plot that are considered noise (label <code>-1</code>)? Why might DBSCAN treat these points as noise, and why is this feature useful compared to K-Means?</li>
</ol>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN, KMeans

# Generate sample data
X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.2, min_samples=5)
db_labels = dbscan.fit_predict(X)

# Apply K-Means
kmeans = KMeans(n_clusters=2, random_state=42)
km_labels = kmeans.fit_predict(X)

# Plot side by side
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# DBSCAN plot
axes[0].scatter(X[:, 0], X[:, 1], c=db_labels, cmap='viridis', s=50)
axes[0].set_title("DBSCAN Clustering")
axes[0].set_xlabel("Feature 1")
axes[0].set_ylabel("Feature 2")

# K-Means plot
axes[1].scatter(X[:, 0], X[:, 1], c=km_labels, cmap='viridis', s=50)
axes[1].set_title("K-Means Clustering")
axes[1].set_xlabel("Feature 1")
axes[1].set_ylabel("Feature 2")

plt.tight_layout()
plt.show()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>The End</h2>
</div>
                </div>
            </div>
        </div>
    </div>

    <footer>
        <p id="footerYear"></p>
    </footer>

    <script src="../../../../../../script.js"></script>

    <script>
        // Note navigation functionality
        document.addEventListener('DOMContentLoaded', function() {
            loadNoteNavigation();
        });
        
        async function loadNoteNavigation() {
            try {
                // Get current page path relative to root
                const currentPath = window.location.pathname;
                const pathParts = currentPath.split('/');
                const fileName = pathParts[pathParts.length - 1];
                
                // Construct the relative path to match navigation data
                let relativePath = '';
                if (pathParts.includes('notes')) {
                    const notesIndex = pathParts.indexOf('notes');
                    relativePath = pathParts.slice(notesIndex).join('/');
                } else {
                    relativePath = 'notes/' + fileName;
                }
                
                // Decode URL encoding (e.g., %20 -> space) to match navigation data
                relativePath = decodeURIComponent(relativePath);
                
                // Load navigation data
                const response = await fetch('../../../../../../notes_navigation.json');
                if (!response.ok) {
                    console.log('Navigation data not found');
                    return;
                }
                
                const navigationData = await response.json();
                const currentNav = navigationData[relativePath];
                
                if (!currentNav) {
                    console.log('Current page not found in navigation data:', relativePath);
                    return;
                }
                
                // Update progress
                const progressText = document.getElementById('progressText');
                const progressFill = document.getElementById('progressFill');
                const progress = ((currentNav.current_index + 1) / currentNav.total_notes) * 100;
                
                progressText.textContent = `${currentNav.current_index + 1} of ${currentNav.total_notes}`;
                progressFill.style.width = `${progress}%`;
                
                // Update current page title in header
                const currentPageTitle = document.getElementById('currentPageTitle');
                const pageTitle = relativePath.split('/').pop().replace('.html', '').replace(/_/g, ' ');
                currentPageTitle.textContent = pageTitle;
                
                // Update previous button
                const prevButton = document.getElementById('prevButton');
                
                if (currentNav.previous) {
                    prevButton.href = '../../../../../../' + currentNav.previous;
                    prevButton.title = 'Previous: ' + currentNav.previous_title;
                    prevButton.style.visibility = 'visible';
                } else {
                    prevButton.style.visibility = 'hidden';
                }
                
                // Update next button
                const nextButton = document.getElementById('nextButton');
                
                if (currentNav.next) {
                    nextButton.href = '../../../../../../' + currentNav.next;
                    nextButton.title = 'Next: ' + currentNav.next_title;
                    nextButton.style.visibility = 'visible';
                } else {
                    nextButton.style.visibility = 'hidden';
                }
                
                // Add keyboard navigation
                document.addEventListener('keydown', function(e) {
                    if (e.ctrlKey || e.metaKey) return; // Don't interfere with browser shortcuts
                    
                    if (e.key === 'ArrowLeft' && currentNav.previous) {
                        window.location.href = '../../../../../../' + currentNav.previous;
                    } else if (e.key === 'ArrowRight' && currentNav.next) {
                        window.location.href = '../../../../../../' + currentNav.next;
                    }
                });
                
            } catch (error) {
                console.error('Error loading navigation:', error);
                document.getElementById('progressText').textContent = 'Navigation unavailable';
            }
        }
    </script>

</body>
</html>