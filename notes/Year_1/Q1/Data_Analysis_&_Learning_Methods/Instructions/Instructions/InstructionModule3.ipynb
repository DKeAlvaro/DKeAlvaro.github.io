{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a12a0f0",
   "metadata": {},
   "source": [
    "# Instruction Session 3\n",
    "## Feature Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this session\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning and Decomposition\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Statistical tools\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import linalg\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap, LogNorm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec73417",
   "metadata": {},
   "source": [
    "## Part 3.1: Principal Component Analysis (PCA)\n",
    "\n",
    "In data science, we often deal with datasets that have many features (high-dimensional data). While having more features can provide richer information, it also makes computation slower, models harder to interpret, and sometimes even hurts performance because of noise and redundancy.  \n",
    "\n",
    "Principal Component Analysis (PCA) is one of the most common techniques to reduce dimensionality while keeping as much of the data’s “essence” as possible — specifically, its variance. In simple terms, PCA finds a new coordinate system for the data that makes it easier to describe using fewer dimensions.\n",
    "\n",
    "Here’s the step-by-step intuition for how PCA works:\n",
    "\n",
    "1. **Compute the covariance matrix**  \n",
    "   We start by computing the covariance matrix of the dataset. This matrix tells us how features vary together — high covariance means two features increase or decrease together.\n",
    "\n",
    "2. **Perform eigendecomposition**  \n",
    "   Next, we use linear algebra to decompose the covariance matrix into **eigenvalues** and **eigenvectors**.  \n",
    "   - The **eigenvectors** define new directions (axes) in which the data varies the most.  \n",
    "   - The **eigenvalues** tell us how much variance lies along each of these directions.\n",
    "\n",
    "3. **Transform the data (center + rotate)**  \n",
    "   Once we have the eigenvectors, we transform the dataset.  \n",
    "   - First, we **center** the data by subtracting its mean (so it is positioned around the origin).  \n",
    "   - Then, we **rotate** the data into the new coordinate system defined by the eigenvectors.  \n",
    "   After this step, the covariance matrix of the transformed data becomes diagonal, meaning the new features are uncorrelated.\n",
    "\n",
    "4. **Select principal components**  \n",
    "   Finally, we can choose just the top few components (those corresponding to the largest eigenvalues), effectively compressing the data while preserving most of its variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7cca0",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1.1: Compute eigenvalues and eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f113fe7",
   "metadata": {},
   "source": [
    "Write a function `compute_eigen(X)` which computes the eigenvalues and eigenvectors of the covariance matrix of some toy dataset `X`. The dataset is a matrix of shape ($N\\times M$), where $N$ denotes the number of data samples and $M$ the dimensionality of the features. The function should return a vector of length $M$ containing the eigenvalues and a matrix of shape ($M\\times M$) containing the corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex11_generate_data()\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = compute_eigen(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56928e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "X = ex11_generate_data()\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = compute_eigen(X)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.2)\n",
    "plot_eigen(np.mean(X, axis=0), eigvals, eigvecs, plt.gca(), width=0.1, color=\"r\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "# print eigenvalues and eigenvectors\n",
    "eigvals, eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757afffc",
   "metadata": {},
   "source": [
    "Have a look at the computed eigenvalues, eigenvectors and the visualization and answer the questions below:\n",
    "- In what direction does the first eigenvector point?\n",
    "- How can the eigenvalue be interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b03e31",
   "metadata": {},
   "source": [
    "\n",
    "Answer: The first eigenvector points in the direction of largest variance. The eigenvalues correspond to the variances along the directions of the eigenvectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c36975",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1.2: Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200a129",
   "metadata": {},
   "source": [
    "Write a function `transform_PCA(X, mean, eigvecs)` which translates some dataset `X` to be centered in the origin, and rotates it, such that its new covariance matrix is diagonal. $X$ is of shape ($N\\times M$), where $N$ denotes the number of data samples and $M$ the dimensionality of the features. The function should return the transformed dataset of shape ($N\\times M$). Also create the function `inversetransform_PCA(X, mean, eigvecs)` which performs the inverse transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Transform data to PCA space\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Transform data back from PCA space\n",
    "\n",
    "\n",
    "    \n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "m = np.mean(X, axis=0)\n",
    "eigvals, eigvecs = compute_eigen(X)\n",
    "Y = transform_PCA(X, m, eigvecs)\n",
    "Z = inversetransform_PCA(Y, m, eigvecs)\n",
    "\n",
    "# plot transformed data\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "ax[0].scatter(Y[:,0], Y[:,1], alpha=0.2)\n",
    "eigvalsY, eigvecsY = compute_eigen(Y)\n",
    "plot_eigen(np.mean(Y, axis=0), eigvalsY, eigvecsY, ax[0], width=0.1, color=\"r\")\n",
    "ax[1].scatter(Z[:,0], Z[:,1], alpha=0.2)\n",
    "eigvalsZ, eigvecsZ = compute_eigen(Z)\n",
    "plot_eigen(np.mean(Z, axis=0), eigvalsZ, eigvecsZ, ax[1], width=0.1, color=\"r\")\n",
    "ax[0].axis(\"equal\"), ax[1].axis(\"equal\")\n",
    "ax[0].grid(), ax[1].grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ac5bb",
   "metadata": {},
   "source": [
    "So far PCA has only been discussed for a toy example. Let's now apply it to high-dimensional data. We will load a dataset containing 400 images of faces. These grayscale images are of size (64 $\\times$ 64) and therefore contain 4096 pixels. In order to process the images, they have been flattened into vectors, which are appended to create a matrix containing 400 images. Below we have plotted the first 100 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485eb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code\n",
    "X = ex13_generate_data()\n",
    "plot_faces(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffcb15",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1.3: Principal components analysis\n",
    "Compute the eigenvalues and vectors of the faces dataset. Plot the first 350 eigenvalues with both a normal as log-scaling on the y-axis.\n",
    "\n",
    "> Note: Since the dataset only contains 400 images, the covariance matrix of size (4096 $\\times$ 4096) is not positive definite (although it should be in theory). Therefore the eigenvalues > 400 are basically useless, however, they are still computed as imaginary quantities. You can plot the real or absolute values of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Plot eigenvalues\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6cdbd",
   "metadata": {},
   "source": [
    "Based on the created plots, what do you observe? How could this be useful for data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d16852",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO  What do you observe?`\n",
    "\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf165e",
   "metadata": {},
   "source": [
    "Now we will use PCA for data compression. Instead of performing the transform to the PCA space with the entire eigenvector matrix with shape (4096 $\\times$ 4096), we only use the $K$ eigenvectors corresponding with the $K$ largest eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb56375",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1.4: Data compression\n",
    "Compress the faces dataset using PCA in a matrix of shape (400 $\\times$ $K$) and then decompress the data and plot the faces using the `plot_faces()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b290786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Data compression\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5786c",
   "metadata": {},
   "source": [
    "Analyze the results. What do you observe if you change $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd7d77",
   "metadata": {},
   "source": [
    "`What do you observe?`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508da1c3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1.5: PCA with Sklearn\n",
    "Compress the faces dataset using PCA class from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe333405",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2f2be",
   "metadata": {},
   "source": [
    "## Part 3.2: Independent component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87ac91",
   "metadata": {},
   "source": [
    "Another approach of finding the most important components in a dataset is independent component analysis (ICA). Below you will use it to analyze a toy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf8ba0",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2.1: Limitations PCA\n",
    "Have a look at the dataset below. Would PCA be a good approach for finding the components of highest variance? Please motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ex21_generate_data()\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.1)\n",
    "plt.grid(True)\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb160d1",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO Limitations PCA`\n",
    "\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790ef81",
   "metadata": {},
   "source": [
    "> Note: From this moment onwards you can use the `FastICA` functions from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67e0b5",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2.2: PCA versus ICA\n",
    "Use the `PCA` and `FastICA` functions from `sklearn` to create the objects `pca_object` and `ica_object`, each with two components. Fit these objects to the dataset and transform the dataset. Save the transformed dataset into the variables `data_transformed_pca` and `data_transformed_ica`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO PCA and ICA\n",
    "# create objects\n",
    "\n",
    "\n",
    "# fit and transform data\n",
    "\n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code\n",
    "_, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "ax[0].scatter(X[:,0], X[:,1], alpha=0.1)\n",
    "ax[1].scatter(data_transformed_pca[:,0], data_transformed_pca[:,1], alpha=0.1)\n",
    "ax[2].scatter(data_transformed_ica[:,0], data_transformed_ica[:,1], alpha=0.1)\n",
    "plot_pca(ax[0], pca_object, np.mean(X, axis=0))\n",
    "plot_ica(ax[0], ica_object, np.mean(X, axis=0))\n",
    "ax[0].grid(True), ax[1].grid(True), ax[2].grid(True)\n",
    "ax[0].axis('equal'), ax[1].axis('equal'), ax[2].axis('equal')\n",
    "ax[0].set_title(\"original data\"), ax[1].set_title(\"transformed data (PCA)\"), ax[2].set_title(\"transformed data (ICA)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a064884",
   "metadata": {},
   "source": [
    "Run your code a couple of times. What do you observe? Which method works best for this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6619d2",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO PCA versus ICA`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efab7d6",
   "metadata": {},
   "source": [
    "#  Wrapper and Filter feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdb938",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.3.1: MI feature selection \n",
    "**run the code and investigate the optimal number of features needed for you the model, what is an optimal trade off? is this method model dependent or model independent, is it filter method or wrapper method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8af786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 0: Load Breast Cancer Dataset\n",
    "# ----------------------------------------------------------------------------\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 0.5: Scale Features (Very Important for Logistic Regression)\n",
    "# ----------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Compute Mutual Information Scores\n",
    "# ----------------------------------------------------------------------------\n",
    "mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=42)\n",
    "feature_ranking = np.argsort(mi_scores)[::-1]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Evaluate Logistic Regression with Top-k Features\n",
    "# ----------------------------------------------------------------------------\n",
    "total_features = X_train.shape[1]\n",
    "accuracy_list = []\n",
    "\n",
    "for k in range(1, total_features + 1):\n",
    "    top_k_features = X_train_scaled.columns[feature_ranking[:k]]\n",
    "    model = LogisticRegression(max_iter=5000, random_state=42)\n",
    "    model.fit(X_train_scaled[top_k_features], y_train)\n",
    "    y_pred = model.predict(X_test_scaled[top_k_features])\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 3: Plot Accuracy vs. Number of Features\n",
    "# ----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(\n",
    "    x=range(1, total_features + 1),\n",
    "    y=accuracy_list,\n",
    "    marker=\"o\",\n",
    "    color=\"purple\"\n",
    ")\n",
    "plt.xlabel(\"Number of Top-k Features (ranked by MI)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression Accuracy vs. Top-k Features (Breast Cancer Dataset)\")\n",
    "plt.xticks(range(1, total_features + 1, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a9029",
   "metadata": {},
   "source": [
    "### Exercise 3.3.2: Recursive feature elimination feature selection \n",
    "**run the following code using the same dataset but with RFE techniuqes for feature selection, how many features would you select, how is it different from MI feature selection, is it model dependent or model independent, is it filter or wrapper method?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364adaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 0: Load Breast Cancer Dataset\n",
    "# ----------------------------------------------------------------------------\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 0.5: Scale Features (Important for Logistic Regression)\n",
    "# ----------------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 1: Recursive Feature Elimination (RFE)\n",
    "# ----------------------------------------------------------------------------\n",
    "total_features = X_train_scaled.shape[1]\n",
    "rfe_accuracy_list = []\n",
    "\n",
    "for k in range(1, total_features + 1):\n",
    "    # Base estimator: Logistic Regression\n",
    "    base_model = LogisticRegression(max_iter=5000, random_state=42)\n",
    "    \n",
    "    # Perform RFE to select top-k features\n",
    "    selector = RFE(estimator=base_model, n_features_to_select=k, step=1)\n",
    "    selector.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Select features chosen by RFE\n",
    "    selected_features = X_train_scaled.columns[selector.support_]\n",
    "    \n",
    "    # Retrain logistic regression only on the selected features\n",
    "    base_model.fit(X_train_scaled[selected_features], y_train)\n",
    "    \n",
    "    # Evaluate accuracy on the test set\n",
    "    y_pred = base_model.predict(X_test_scaled[selected_features])\n",
    "    rfe_accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Step 2: Plot Accuracy vs. Number of Selected Features\n",
    "# ----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(\n",
    "    x=range(1, total_features + 1),\n",
    "    y=rfe_accuracy_list,\n",
    "    marker=\"o\",\n",
    "    color=\"green\"\n",
    ")\n",
    "plt.xlabel(\"Number of Selected Features (RFE)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression Accuracy vs. Selected Features (RFE)\")\n",
    "plt.xticks(range(1, total_features + 1, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736415b2",
   "metadata": {},
   "source": [
    "# the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340bee5",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
