{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c53144",
   "metadata": {},
   "source": [
    "# Instruction Session 5\n",
    "\n",
    "In this session, we will explore and compare different clustering techniques.  \n",
    "The focus will be on understanding their assumptions, strengths, and limitations.  \n",
    "\n",
    "We will cover the following algorithms:\n",
    "\n",
    "1. **K-Means** – a centroid-based algorithm that partitions data into clusters of roughly spherical shape.  \n",
    "2. **Gaussian Mixture Models (GMMs)** – a probabilistic approach that assumes data is generated from a mixture of Gaussian distributions.  \n",
    "3. **Self-Organizing Maps (SOMs)** – a neural network–based technique that projects high-dimensional data into a lower-dimensional (typically 2D) grid.  \n",
    "4. **DBSCAN** – a density-based algorithm that can discover arbitrarily shaped clusters and identify noise/outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "\n",
    "from helpers import *\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Decomposition\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Statistical tools\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import linalg\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap, LogNorm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350e702",
   "metadata": {},
   "source": [
    "## Part 1: K-means algorithm\n",
    "In this part we will discuss unsupervised machine learning problems and describe how the K-means algorithm can be used to solve these.\n",
    "\n",
    "Unsupervised machine learning problems are problems in which we try to determine some particular structure within a data set. On the contrary, supervised machine learning problems require us to model some kind of input-output mapping. Unsupervised machine learning problems do not have a specified output which we would like to model. Instead we are interesting in making sense of the data in grouping this data, without knowing beforehand which and how many groups exist.\n",
    "\n",
    "The K-means algorithm can group $N$ data samples of dimension $D$ into $K$ groups or clusters. These clusters can each be characterized by their mean vector: the expected or average value of the points which are assigned to the cluster. The mean vector denoting the center of the $k^{th}$ cluster can be represented as the column vector ${\\bf{\\mu}}^{(k)} = [\\mu_1^{(k)},\\ \\mu_2^{(k)},\\ \\ldots, \\mu_D^{(k)}]^\\top$ and the $n^{th}$ data sample can be represented by the column vector ${\\bf{x}}^{(n)} = [x_1^{(n)},\\ x_2^{(n)},\\ \\ldots, x_D^{(n)}]^\\top$, where the superscript denotes the sample index.\n",
    "\n",
    "The K-means algorithm tries to minimize the (within-cluster) Euclidean squared distance\n",
    "$$J({\\bf{X}}, {\\bf{\\mu}}) = \\frac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^K \\rho_k^{(n)} \\| {\\bf{x}}^{(n)} - {\\bf{\\mu}}^{(k)}\\|^2$$\n",
    "Here $\\rho_k^{(n)}$ is a so-called indicator function that is defined as \n",
    "$$ \\rho_k^{(n)} = \\begin{cases} 1 & \\text{if sample }{\\bf{x}}^{(n)}\\text{ is assigned to cluster }k \\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "This indicator function equals $1$ when the corresponding data point is assigned to the corresponding cluster and $0$ otherwise. The cost function therefore represents the average squared distance with respect to the cluster that a point is assigned to.\n",
    "\n",
    "The algorithm is specified as follows:\n",
    "\n",
    "1. Initialize means ${\\bf{\\mu}}$.\n",
    "2. Assign data points to closest cluster mean (i.e. update $\\rho_k^{(n)}$).\n",
    "3. Calculate new cluster means as the average values of the points that are assigned to it (i.e. update ${\\bf{\\mu}}$).\n",
    "4. Calculate cost function $J({\\bf{X}}, {\\bf{\\mu}})$.\n",
    "5. If not converged, go back to 2 and repeat.\n",
    "\n",
    "Here we will describe the algorithm in words. First the centers of the clusters are initialized. This can be done arbitrarily, but often the centers are set to random (but distinct) samples of the data set.\n",
    "Once the means are set, we assign each data sample to the cluster that is closest to it. In order to do so, we calculate the Euclidean squared distance between a point and all the clusters and find the cluster that is closest to it. We repeat this for all points and we therefore completely specify $\\rho_k^{(n)}$. Once all points have been assigned to a cluster, we look up all points corresponding to a certain cluster and we average these to calculate the new cluster center. We update all cluster means. Then we evaluate the current fit of the clusters on the data by evaluate the cost function. If we still see a significant improvement in the cost function, we repeat updating the assignments and cluster centers and if the cost function seems to have converged, we stop iterating.\n",
    "\n",
    "In this part of the assignment you will implement the K-means algorithm from scratch, starting with the initialization of the cluster means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f4d8d",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.1: Initializing cluster centers\n",
    "Consider the function `X = ex3_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `means = initialize_means(X, K)` that accepts the data set ${\\bf{X}}$  and number of clusters $K$ as input and returns a matrix of shape (K x D), representing the vertical concatenation of $K$ transposed mean vectors of dimension $D$. These means should be initialized such that they coincide with *random* samples from the data set, which are always *distinct*. In other words, the means should equal a random subset of the availabe data set, where no means are equal. Also keep in mind that the number of clusters is variable in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the function initialize_means(X, K)\n",
    "\n",
    "def initialize_means(X, K):\n",
    "\n",
    "    # get number of samples \n",
    "    N = np.shape(X)[0]\n",
    "\n",
    "    # fetch random indices of data\n",
    "    inds = np.arange(N)\n",
    "    np.random.shuffle(inds)\n",
    "    inds = inds[:K]\n",
    "\n",
    "    # fetch means\n",
    "    means = X[inds,:]\n",
    "\n",
    "    # return means\n",
    "    return means\n",
    "    \n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10)\n",
    "plt.scatter(means[:,0], means[:,1], c=\"red\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde4186",
   "metadata": {},
   "source": [
    "### End of exercise 1.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e70186",
   "metadata": {},
   "source": [
    "Now that the clusters have been initialized, it is time to assign points to the closest clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3961646",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.2: Assign points to clusters\n",
    "Again consider the function `X = ex4_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `rho = assign_data_to_clusters(X, means)` that accepts the data set ${\\bf{X}}$ and matrix of means ${\\bf{\\mu}}$ as input and returns a matrix of shape (N x K), which contains all indicator functions $\\rho_k^{(n)}$. This matrix should be a matrix of only ones and zeros and each row should sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the function assign_data_to_clusters(X, means)\n",
    "\n",
    "def assign_data_to_clusters(X, means):\n",
    "\n",
    "    # fetch dimensions \n",
    "    N = np.shape(X)[0]\n",
    "    K = np.shape(means)[0]\n",
    "\n",
    "    # allocate array of rho\n",
    "    rho = np.zeros((N, K))\n",
    "\n",
    "    # loop through samples\n",
    "    for n in range(N):\n",
    "\n",
    "        # allocate array for distances\n",
    "        J = np.zeros(K)\n",
    "\n",
    "        # loop through clusters\n",
    "        for k in range(K):\n",
    "\n",
    "            # calculate distance\n",
    "            J[k] = np.linalg.norm(X[n,:] - means[k,:])**2\n",
    "\n",
    "        # select closest clusters and put rho to 1\n",
    "        rho[n, np.argmin(J)] = 1\n",
    "\n",
    "    # return rho\n",
    "    return rho\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# assign point to clusters\n",
    "rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], c=\"red\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fff7e4",
   "metadata": {},
   "source": [
    "### End of exercise 1.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58828216",
   "metadata": {},
   "source": [
    "The means have been initialized, the point have been assigned to a cluster. Now the cluster centers can be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd78c69",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.3: Update cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67763b57",
   "metadata": {},
   "source": [
    "Again consider the function `X = ex3_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ data vectors of dimension $D$. Create a function `means = update_cluster_centers(X, rho)` that accepts the data set ${\\bf{X}}$ and matrix of indicators $\\rho$ as input and returns a matrix of shape (K x D), which contains the new cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ec4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Complete the function update_cluster_centers(X, rho)\n",
    "def update_cluster_centers(X, rho):\n",
    "\n",
    "    # fetch dimensions \n",
    "    (N, K) = np.shape(rho)\n",
    "    D = np.shape(X)[1]\n",
    "\n",
    "    # allocate new means\n",
    "    means = np.zeros((K,D))\n",
    "\n",
    "    # loop through clusters\n",
    "    for k in range(K):\n",
    "\n",
    "        # update means\n",
    "        means[k,:] = np.mean(X[np.argmax(rho,axis=1)==k,:], axis=0)\n",
    "\n",
    "\n",
    "    # return means\n",
    "    return means\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# assign point to clusters\n",
    "rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "# update means\n",
    "means_new = update_cluster_centers(X, rho)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "plt.scatter(means_new[:,0], means_new[:,1], 50, c=\"blue\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246474f2",
   "metadata": {},
   "source": [
    "### End of exercise 1.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c8dae",
   "metadata": {},
   "source": [
    "Almost there! Now it is just a matter of combining the previous functions for finalizing the K-means algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9325a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.4: Implement K-means algorithm\n",
    "\n",
    "First create a function `J = Kmeans_loss(X, means, rho)` that calculates the within-cluster Euclidean squared distance as defined above. Secondly create the final `means, rho, J = Kmeans(X, K)` function that combines all previous functions to create the K-means algorithm as specified in the the introduction of this part of the assignment. This function returns the final cluster centers, the indicator function and a history of the losses. Save the loss *after* each iteration and stop iterating when the difference in loss does no longer exceed 1e-10. The initial loss based on the randomly initialized means should not be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the Kmeans_loss(X, means, rho) function\n",
    "\n",
    "def Kmeans_loss(X, means, rho):\n",
    "\n",
    "    # get dimensions\n",
    "    (K, D) = np.shape(rho)\n",
    "    N = np.shape(X)[0]\n",
    "\n",
    "    # initialize loss function\n",
    "    J = 0\n",
    "\n",
    "    # loop through samples\n",
    "    for n in range(N):\n",
    "\n",
    "        J += 1/N*np.linalg.norm(X[n,:] - means[np.argmax(rho, axis=1)[n],:])**2\n",
    "\n",
    "    # return loss \n",
    "    return J\n",
    "    \n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Complete the Kmeans(X, K) function\n",
    "def Kmeans(X, K):\n",
    "\n",
    "    # initialize means\n",
    "    means = initialize_means(X, K)\n",
    "\n",
    "    # perform assignment\n",
    "    rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "    # initialize loss\n",
    "    J = np.append([1e10], Kmeans_loss(X, means, rho))\n",
    "\n",
    "    # iterate until convergence\n",
    "    while np.abs(J[-1] - J[-2]) > 1e-10:\n",
    "\n",
    "        # update means\n",
    "        means = update_cluster_centers(X, rho)\n",
    "\n",
    "        # perform assignment\n",
    "        rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "        # calculate loss\n",
    "        J = np.append(J, Kmeans_loss(X, means, rho))\n",
    "\n",
    "    # return parameters\n",
    "    return means, rho, J[1:]\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means, rho, J = Kmeans(X, 3)\n",
    "\n",
    "# plot data\n",
    "_,ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "ax[0].scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "ax[0].scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "ax[1].plot(J)\n",
    "ax[0].grid(), ax[1].grid(), ax[1].set_ylabel(\"cost function\"), ax[1].set_xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1ddf9",
   "metadata": {},
   "source": [
    "### End of exercise 1.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cbc19",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.5: Number of clusters\n",
    "In the previous assignment the data had been generate from 3 clusters. In practice the number of clusters is often unknown. In this exercise we will see what happens when we add extra clusters. In this exercise, run your K-means algorithm on the previous data set for 2 up to and including 10 clusters and save the final value of the loss (i.e. the loss value when the algorithm has converged).\n",
    "\n",
    "Plot the final loss against the number of used clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b84cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306accff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Plot the Kmeans loss against the number of clusters\n",
    "\n",
    "# perform K means\n",
    "J_mem = np.zeros(9)\n",
    "for k in range(2,11):\n",
    "    _, _, J = Kmeans(X, k)\n",
    "    J_mem[k-2] = J[-1]\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.plot(range(2,11), J_mem)\n",
    "plt.grid(), plt.ylabel(\"cost function\"), plt.xlabel(\"number of clusters\");\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80fa892",
   "metadata": {},
   "source": [
    "### End of exercise 1.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7a4df",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.6: Shortcoming of the K-means algorithm\n",
    "Apply the Kmeans algorithm for the new data set generate by `X = ex36_generate_data()`. Visualise the data and come up with an appropriate number of clusters. Plot the data points in a scatter plot, plot the means as red crosses in the same plot and color the data point according to their assigned cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aef1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex36_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Plot clusters of new data set\n",
    "\n",
    "# initialize means\n",
    "means, rho, J = Kmeans(X, 3)\n",
    "\n",
    "# visualize results\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "plt.grid()\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc306b7",
   "metadata": {},
   "source": [
    "### End of exercise 1.6\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02135122",
   "metadata": {},
   "source": [
    "## Part 2: Gaussian Mixture modeling\n",
    "The K-Means algorithm is a fast and simple method that works well for many applications. However, due to its simplicity, it isn't suitable for all situations. Fuzzy C-Means (FCM) improves on K-Means by allowing for soft clustering, where data points can belong to multiple clusters with varying degrees of membership. While this flexibility is advantageous, FCM has its own set of limitations. It can be computationally intensive, especially for large datasets, and is sensitive to the initial selection of cluster centers. Additionally, FCM may struggle with outliers, varying cluster sizes, and high-dimensional data, which can lead to suboptimal clustering results.\n",
    "\n",
    "In this part we present another methodology for clustering data, namely through Gaussian mixture modeling. In this approach we do not rely on a deterministic algorithm for determining the cluster means and assignments, but instead we model the data set by a probability density function.\n",
    "\n",
    "We will assume that the data set has been generated from a Gaussian mixture model, which is formally specified as\n",
    "$$ p({\\bf{x}}^{(n)}) = \\sum_{k=1}^K \\rho_k \\mathcal{N}({\\bf{x}}^{(n)} \\mid {\\bf{\\mu}}_k, \\Sigma_k),$$\n",
    "where a data sample ${\\bf{x}}^{(n)}$ is originating from a Gaussian mixture model with $K$ individual Gaussian distributions with means ${\\bf{\\mu}}_k$ and covariance matrices ${\\bf{\\Sigma}}_k$. The mean denotes the center or mode of the Gaussian distribution and the covariance matrix specifies the strech and tilt of the Gaussian distribution. In this model the mixing coefficients $\\rho_k$ specify how much each of the Gaussian distributions contributes in the model. Because the Gaussian mixture model is a probability density function, integrating over ${\\bf{x}}$ should always equal 1. Because the individual Gaussians already satisfy this requirement, the mixing coefficients are constrained by\n",
    "$$ \\sum_{k=1}^K \\rho_k = 1.$$\n",
    "To give some intuition on this model, we give a 1-dimensional example below. Here we model a data set by a mixture of 2 Gaussians. The individual *weighted* Gaussian distributions are colored in blue and the corresponding mixture model distribution is colored in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4_plot_GMM_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77316e",
   "metadata": {},
   "source": [
    "During this part of the assignment you will implement the so-called Expectation-Maximization (EM) algorithm for learning the Gaussian mixture model. This algorithm consists of two step, the expectation step (E-step) and the maximization step (M-step). The exact details of the algorithm are beyond the scope of this assignment, but here we will present the update equations for these steps.\n",
    "\n",
    "The EM algorithm works as follows:\n",
    "\n",
    "1. Initialize the means ${\\bf{\\mu}}_k$, covariances $\\Sigma_k$ and mixing coefficients $\\rho_k$. Often the means are initialized using the Kmeans algorithm. The covariance matrices can be set to identity matrices and the mixing coefficients can be initialized to the fraction of points assigned to the cluster with Kmeans divided by the total number of samples.\n",
    "2. *Expectation step*: evaluate the responsibilities $\\gamma_{nk}$ using the current parameter values as \n",
    "$$ \\gamma_{nk} = \\frac{\\rho_k \\mathcal{N}({\\bf{x}}_n \\mid {\\bf{\\mu}}_k, \\Sigma_k)}{\\sum_{j=1}^K \\rho_j \\mathcal{N}({\\bf{x}}_n \\mid {\\bf{\\mu}}_j, \\Sigma_j)}$$\n",
    "3. *Maximization step*: re-estimate the parameters using the current responsibilities\n",
    "$$ {\\bf{\\mu}}_k^\\text{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk}{\\bf{x}}_n $$\n",
    "$$ \\Sigma_k^\\text{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk} ({\\bf{x}}_n - {\\bf{\\mu}}_k^\\text{new})({\\bf{x}}_n - {\\bf{\\mu}}_k^\\text{new})^\\top $$\n",
    "$$ \\rho_k = \\frac{N_k}{N} $$\n",
    "where $N$ denotes the number of samples and where\n",
    "$$ N_k = \\sum_{n=1}^N \\gamma_{nk}$$\n",
    "4. Evaluate the log-likelihood\n",
    "$$ \\ln p({\\bf{X}} \\mid {\\bf{\\mu}}, \\Sigma, {\\bf{\\rho}}) = \\sum_{n=1}^N \\ln \\left\\{ \\sum_{k=1}^K \\rho_k \\mathcal{N}({\\bf{x}}^{(n)} \\mid {\\bf{\\mu}}_k, \\Sigma_k)\\right\\}$$\n",
    "\n",
    "It is important to grasp what is going on in this algorithm. The responsibilities $\\gamma_{nk}$ are similar to the indicator functions from the Kmeans algorithm. However, where the Kmeans algorithm performs a hard clustering (each point can be assigned to only 1 cluster), the Gaussian mixture model allows for a soft clustering (each point can be modeled by both Gaussian distribution, but just to a different extent). The indicator function of the Kmeans algorithm was one-hot coded, meaning that a point was assigned to 1 cluster only. The responsibilities $\\gamma_{nk}$ specify how likely a data sample ${\\bf{x}}_n$ is to be generated from a cluster. With a Gaussian mixture model a point can therefore be assigned to different extents to multiple clusters. The expectation step calculates these responsibilities and the division in this expression makes sure that all rows sum op to 1.\n",
    "\n",
    "In the maximization step the parameters are updated. Here the contribution of each data sample towards the parameters depends on the corresponding responsibilities. This means that a point that is very likely to have originated from a certain cluster will have a high influence on the statistics of that cluster. The variable $N_k$ specifies how many points are located to a certain Gaussian distribution. Because this parameter is the summation over the individual responsibilities, $N_k$ is not forced to be an integer.\n",
    "\n",
    "The log-likelihood is a cost function which takes the variances and uncertainties in our model into account. It describes the probability of that data set being generated from a Gaussian mixture model. To prevent numerical instability we commonly use the log-likelihood instead of the normal likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60623a79",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.1: Initialize clusters\n",
    "Consider the function from the previous part `X = ex46_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `means, covs, rho = initialize_GMM(X, K)` that accepts the data set ${\\bf{X}}$ as input and returns the following in this order:\n",
    "- `means`: a matrix of size (K x D) that contains the initial cluster means, as a vertical concatenation of the transposed mean vectors. These means should be initialized using the previously written K-means algorithm.\n",
    "- `covs`: a matrix of size (K x D x D) that contains the covariance matrices of the initial clusters. Each matrix `covs[k,:,:]` represents the covariance matrix of the $k^\\text{th}$ cluster. Initialize these covariance matrices as identity matrices.\n",
    "- `rho`: a vector of length K that contains the mixing coefficients as specified above. Initialize this vector based on the indicator function returned by the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the initialize_GMM(X, K) function\n",
    "\n",
    "\n",
    "    \n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "X = ex36_generate_data()\n",
    "\n",
    "# initialize GMM\n",
    "means, covs, rho = initialize_GMM(X, 2)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad0d32",
   "metadata": {},
   "source": [
    "### End of exercise 2.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1721e21",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.2: Expectation step\n",
    "Create a function `gamma = expectation_step(X, means, covs, rho)` that accepts the data set, means, covariances and mixing coefficients with dimensions specified above. This function should perform the expectation step and should return the calculated responsibilities as defined above as a matrix of size (N x K) where each row corresponds to the assignment fraction of a sample amongst the different clusters. Make sure this matrix is properly normalized such that the elements in each row add up to 1. Use the `multivariate_normal` function that has been imported from `scipy.stats` at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Complete the expectation_step(X, means, covs, rho) function\n",
    "\n",
    "\n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27444",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = expectation_step(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90be901",
   "metadata": {},
   "source": [
    "### End of exercise 2.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249cbc3a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.3: Maximization step\n",
    "Create a function `means, covs, rho = maximization_step(X, gamma)` that accepts the data set and responsibilities with dimensions specified above. This function should perform the maximization step and should return the new means, covariances and mixing coefficients with dimensions as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the maximization_step(X, gamma) function\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c701d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximization step\n",
    "means, covs, rho = maximization_step(X, gamma)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c7f95",
   "metadata": {},
   "source": [
    "### End of exercise 2.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0ee38",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.4: Log-likelihood calculation\n",
    "Create a function `J = loglikelihood(X, means, covs, rho)` that accepts the data set, means, covariance matrices and mixing coefficients with dimensions specified above. This function should calculate and return the log-likelihood of the data under the specified Gaussian mixture model. Use the definition as specified in the beginning of Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ccb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the loglikelihood(X, means, covs, rho) function\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262aac0",
   "metadata": {},
   "source": [
    "### End of exercise 2.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c8f46",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.5: Gaussian mixture modeling\n",
    "Now that all the subfunctions have been defined it is time to tie them together and to form a function which does the Gaussian mixture modelling. Create a function `means, covs, rho, gamma, J = GMM_modeling(X, K, nr_iterations)` that does the following:\n",
    "\n",
    "1. Initialize the parameters of the Gaussian mixture model.\n",
    "2. Performs `nr_iterations` iterations of the following:\n",
    "    1. Perform the expectation step.\n",
    "    2. Perform the maximization step.\n",
    "    3. Calculate the log-likelihood.\n",
    "3. returns the parameters and a vector of saved values of the log-likelihood.\n",
    "\n",
    "The function should return all the parameters of the trained Gaussian mixture model, containing the final means, covariance matrices, mixing coefficients, responsibilities and a vector containing all calculated values of the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac81f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Complete the GMM_modeling(X, K, nr_iterations) function\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445067f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GMM\n",
    "means, covs, rho, gamma, J = GMM_modeling(X, 2, 10)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(J)\n",
    "plt.grid(), plt.xlabel(\"iteration\"), plt.ylabel(\"log-likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa498d8d",
   "metadata": {},
   "source": [
    "### End of exercise 2.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85d78b",
   "metadata": {},
   "source": [
    "## Part 3: Kohonen Maps or Self Organizing Maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd41353",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: Self Organizing maps for outlier detection\n",
    "In this task, we will capitalize the power of outlier detection to isolate the outliers from a toy dataset, run the following code to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba24cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "outliers_percentage = 0.35\n",
    "inliers = 300\n",
    "outliers = int(inliers * outliers_percentage)\n",
    "\n",
    "\n",
    "data = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[.3, .3],\n",
    "                  n_samples=inliers, random_state=0)[0]\n",
    "\n",
    "data = scale(data)\n",
    "data = np.concatenate([data, \n",
    "                       (np.random.rand(outliers, 2)-.5)*4.])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5dcf7",
   "metadata": {},
   "source": [
    "What we expect from a good outlier algorithm is that all the samples far away from the two main clusters are labeled as outliers. This can be obtained considering as outliers the samples with a high quantization error.\n",
    "\n",
    "**To test this idea you have the code to 1) train a SOM, 2) compute the quantization error, 3) set a treshold for the quantization error**\n",
    "\n",
    "**Can you adjust the threshold such that you isolate the outliers in the plot?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ce395",
   "metadata": {},
   "outputs": [],
   "source": [
    "som = MiniSom(2, 1, data.shape[1], sigma=1, learning_rate=0.5,\n",
    "              neighborhood_function='triangle', random_seed=10)\n",
    "\n",
    "\n",
    "som.train(data, 100, random_order=False, verbose=True)  # random training\n",
    "\n",
    "# the Euclidean distance between each data point and its closest BMU (best matching unit) in the SOM\n",
    "quantization_errors = np.linalg.norm(som.quantization(data) - data, axis=1)\n",
    "# np.percentile(data, p) Returns the value below which p% of the data lies.\n",
    "outliers_percentage = 0.10\n",
    "error_treshold = np.percentile(quantization_errors, \n",
    "                               100*(1-outliers_percentage))\n",
    "\n",
    "print('Error treshold:', error_treshold)\n",
    "\n",
    "is_outlier = quantization_errors > error_treshold\n",
    "\n",
    "plt.hist(quantization_errors)\n",
    "plt.axvline(error_treshold, color='k', linestyle='--')\n",
    "plt.xlabel('error')\n",
    "plt.ylabel('frequency')\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data[~is_outlier, 0], data[~is_outlier, 1],\n",
    "            label='inlier')\n",
    "plt.scatter(data[is_outlier, 0], data[is_outlier, 1],\n",
    "            label='outlier')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7926f",
   "metadata": {},
   "source": [
    "## Part 4: DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c36e76",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.1: Comparing K-means and DBSCAN\n",
    "\n",
    "Run the following code and answer the questions:\n",
    "1. **Cluster Shapes:**  \n",
    "   Looking at the DBSCAN and K-Means plots, which algorithm correctly identified the crescent-shaped clusters in the `make_moons` dataset? Explain why one algorithm succeeded while the other struggled.\n",
    "\n",
    "2. **Noise and Outliers:**  \n",
    "   DBSCAN can label some points as noise. Are there any points in the DBSCAN plot that are considered noise (label `-1`)? Why might DBSCAN treat these points as noise, and why is this feature useful compared to K-Means?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaea3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "# Generate sample data\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "db_labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "km_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# DBSCAN plot\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=db_labels, cmap='viridis', s=50)\n",
    "axes[0].set_title(\"DBSCAN Clustering\")\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# K-Means plot\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=km_labels, cmap='viridis', s=50)\n",
    "axes[1].set_title(\"K-Means Clustering\")\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dc355",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4642f8",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
