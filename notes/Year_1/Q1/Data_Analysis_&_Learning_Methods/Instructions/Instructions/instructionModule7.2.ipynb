{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkAIUxi52Cii"
      },
      "source": [
        "# Naive Bayes Classification \n",
        "\n",
        "## Theory\n",
        "\n",
        "Naive Bayes is a **probabilistic classifier** based on **Bayes' theorem** with a strong assumption that features are **conditionally independent** given the class. Despite this \"naive\" assumption, it often performs surprisingly well in practice.\n",
        "\n",
        "Bayes' theorem is given by:\n",
        "\n",
        "$P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}$\n",
        "\n",
        "Where:  \n",
        "- $P(C|X)$ is the **posterior probability** of class $C$ given features $X$.  \n",
        "- $P(X|C)$ is the **likelihood** of observing features $X$ given class $C$.  \n",
        "- $P(C)$ is the **prior probability** of class $C$.  \n",
        "- $P(X)$ is the **evidence**, i.e., probability of observing $X$ (constant across classes).\n",
        "\n",
        "### Gaussian Naive Bayes\n",
        "\n",
        "When features are continuous, we assume each feature follows a **Gaussian (Normal) distribution** for each class. The probability density function is:\n",
        "\n",
        "$P(x_i | C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp\\left( - \\frac{(x_i - \\mu_C)^2}{2 \\sigma_C^2} \\right)$\n",
        "\n",
        "Where:  \n",
        "- $x_i$ is the feature value.  \n",
        "- $\\mu_C$ is the mean of the feature for class $C$.  \n",
        "- $\\sigma_C^2$ is the variance of the feature for class $C$.\n",
        "\n",
        "The **class with the highest posterior probability** is assigned to the sample:\n",
        "\n",
        "$\\hat{C} = \\arg\\max_C P(C|X) = \\arg\\max_C P(C) \\prod_i P(x_i|C)$\n",
        "\n",
        "### Steps in Gaussian Naive Bayes:\n",
        "\n",
        "1. **Separate the training data** by class.  \n",
        "2. **Compute mean and variance** for each feature per class.  \n",
        "3. **Calculate likelihoods** using the Gaussian formula.  \n",
        "4. **Multiply likelihoods with class prior** to get class probabilities.  \n",
        "5. **Predict the class** with the highest posterior probability.\n",
        "\n",
        "**Key Assumption:** Features are conditionally independent given the class.  \n",
        "\n",
        "**Advantages:**  \n",
        "- Simple, fast, and effective.  \n",
        "- Works well even with small datasets.  \n",
        "\n",
        "**Limitations:**  \n",
        "- Independence assumption may not hold in real data.  \n",
        "- Sensitive to irrelevant features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GKyWuO343Wm"
      },
      "source": [
        "# Gaussian Naive Bayes Step-by-Step Example\n",
        "\n",
        "We illustrate **Gaussian Naive Bayes** on a toy dataset with **continuous features** and **binary classes**.\n",
        "\n",
        "| Feature 1 (X1) | Feature 2 (X2) | Class |\n",
        "|----------------|----------------|-------|\n",
        "| 1.0            | 2.0            | A     |\n",
        "| 1.2            | 1.8            | A     |\n",
        "| 2.0            | 3.0            | B     |\n",
        "| 2.2            | 3.2            | B     |\n",
        "\n",
        "We want to **predict the class** for a new sample:  \n",
        "**X = [1.1, 2.1]**\n",
        "\n",
        "---\n",
        "\n",
        "## Step 0: Outline\n",
        "\n",
        "1. Compute **prior probabilities** for each class.  \n",
        "2. Compute **likelihoods** for each feature (for Gaussian, use mean & variance).  \n",
        "3. Multiply likelihoods and prior to get **unnormalized posterior**.  \n",
        "4. **Normalize** posteriors so they sum to 1.  \n",
        "5. Choose the class with the **highest posterior probability**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Compute class priors\n",
        "\n",
        "Count of each class:\n",
        "\n",
        "- Class A: 2  \n",
        "- Class B: 2  \n",
        "- Total: 4  \n",
        "\n",
        "$$\n",
        "P(A) = \\frac{2}{4} = 0.5, \\quad P(B) = \\frac{2}{4} = 0.5\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: Compute mean and variance for each feature per class\n",
        "\n",
        "**Class A:**  \n",
        "- Feature 1: $\\mu_1 = 1.1$, $\\sigma_1^2 = 0.01$  \n",
        "- Feature 2: $\\mu_2 = 1.9$, $\\sigma_2^2 = 0.01$  \n",
        "\n",
        "**Class B:**  \n",
        "- Feature 1: $\\mu_1 = 2.1$, $\\sigma_1^2 = 0.01$  \n",
        "- Feature 2: $\\mu_2 = 3.1$, $\\sigma_2^2 = 0.01$  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 3: Compute Gaussian probability density for each feature\n",
        "\n",
        "Gaussian PDF formula:  \n",
        "\n",
        "$$\n",
        "P(x_i \\mid C) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "\n",
        "### Class A\n",
        "\n",
        "- Feature 1: $P(X_1=1.1 \\mid A) \\approx 3.989$  \n",
        "- Feature 2: $P(X_2=2.1 \\mid A) \\approx 0.054$  \n",
        "\n",
        "### Class B\n",
        "\n",
        "- Feature 1: $P(X_1=1.1 \\mid B) \\approx 1.87 \\cdot 10^{-11}$  \n",
        "- Feature 2: $P(X_2=2.1 \\mid B) \\approx 1.87 \\cdot 10^{-11}$  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 4: Compute unnormalized posterior (likelihood Ã— prior)\n",
        "\n",
        "$$\n",
        "\\tilde{P}(A \\mid X) = P(A) \\cdot P(X_1 \\mid A) \\cdot P(X_2 \\mid A) \\approx 0.5 \\cdot 3.989 \\cdot 0.054 \\approx 0.108\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\tilde{P}(B \\mid X) = P(B) \\cdot P(X_1 \\mid B) \\cdot P(X_2 \\mid B) \\approx 0.5 \\cdot (1.87 \\cdot 10^{-11})^2 \\approx 1.75 \\cdot 10^{-22}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5: Normalize to get actual posterior probabilities\n",
        "\n",
        "Normalization ensures the probabilities sum to 1:\n",
        "\n",
        "$$\n",
        "P(A \\mid X) = \\frac{\\tilde{P}(A \\mid X)}{\\tilde{P}(A \\mid X) + \\tilde{P}(B \\mid X)} \\approx \\frac{0.108}{0.108 + 1.75 \\cdot 10^{-22}} \\approx 1.0\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(B \\mid X) = \\frac{\\tilde{P}(B \\mid X)}{\\tilde{P}(A \\mid X) + \\tilde{P}(B \\mid X)} \\approx \\frac{1.75 \\cdot 10^{-22}}{0.108 + 1.75 \\cdot 10^{-22}} \\approx 0.0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6: Predict class\n",
        "\n",
        "Since $P(A \\mid X) > P(B \\mid X)$, we **predict class = A** for the sample $X=[1.1,2.1]$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "actWA9ew_cg7"
      },
      "source": [
        "## Exercise: Implement Gaussian Naive Bayes for the Iris Dataset \n",
        "\n",
        "In this exercise, you will manually implement Gaussian Naive Bayes using the Iris dataset. You will **NOT** use sklearn's Naive Bayes classifier; instead, you will follow the step-by-step process of computing probabilities, likelihoods, and predictions.\n",
        "\n",
        "You can use the following Python modules:\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Step 0: Load the dataset\n",
        "1. Load the Iris dataset using `load_iris()`\n",
        "2. Store the features in `X` and the target labels in `y`  \n",
        "3. Split the dataset into train and test sets using `train_test_split()` (use `test_size=0.3` and `random_state=42`)\n",
        "\n",
        "**Hint:** Use `iris.data` for features and `iris.target` for labels\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Create the training function\n",
        "Create a function `train_naive_bayes(X_train, y_train)` that:\n",
        "- Gets unique classes using `np.unique(y_train)`\n",
        "- For each class, finds all samples belonging to that class\n",
        "- Calculates and stores: mean, variance, and prior probability\n",
        "\n",
        "**Hints:**\n",
        "- Use `X_train[y_train == cls]` to get samples for a specific class\n",
        "- Use `np.mean(class_data, axis=0)` for mean of each feature\n",
        "- Use `np.var(class_data, axis=0)` for variance of each feature\n",
        "- Prior = number of samples in class / total samples\n",
        "\n",
        "**Return:** A dictionary {} with statistics for each class\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: Create the prediction function\n",
        "Create a function `predict_sample(x, stats)` that:\n",
        "- Loops through each class in the statistics\n",
        "- Calculates likelihood using the Gaussian formula\n",
        "- Multiplies likelihood by prior probability\n",
        "- Returns the class with highest probability\n",
        "\n",
        "**Gaussian Formula:**\n",
        "```python\n",
        "likelihood = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(-0.5 * ((x - mean)**2) / variance)\n",
        "```\n",
        "\n",
        "**Hints:**\n",
        "- Use `np.prod()` to multiply likelihoods of all features\n",
        "- Keep track of best probability and best class\n",
        "- Return the class with the highest probability\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3: Train your model\n",
        "- Call `train_naive_bayes(X_train, y_train)` to get model statistics\n",
        "- Store the result in a variable called `model`\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4: Make predictions\n",
        "- Use list comprehension to predict each sample in the test set\n",
        "- `predictions = [predict_sample(x, model) for x in X_test]`\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5: Calculate accuracy\n",
        "- Use `np.mean(predictions == y_test)` to calculate accuracy\n",
        "- Print the result\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6: Print model details \n",
        "Print the statistics for each class to understand what the model learned:\n",
        "- Prior probability\n",
        "- Mean of each feature  \n",
        "- Variance of each feature\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Output Structure:\n",
        "```\n",
        "Accuracy: 0.XX\n",
        "\n",
        "Model Statistics:\n",
        "Class 0: Prior = 0.XXX\n",
        "  Mean: [X.XX X.XX X.XX X.XX]\n",
        "  Variance: [X.XX X.XX X.XX X.XX]\n",
        "\n",
        "Class 1: Prior = 0.XXX\n",
        "  Mean: [X.XX X.XX X.XX X.XX]  \n",
        "  Variance: [X.XX X.XX X.XX X.XX]\n",
        "\n",
        "Class 2: Prior = 0.XXX\n",
        "  Mean: [X.XX X.XX X.XX X.XX]\n",
        "  Variance: [X.XX X.XX X.XX X.XX]\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "esvoqzb7AiBJ",
        "outputId": "69918dad-6b25-496d-fcfa-fc70affef4e0"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A simplified NV implementation using Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ImrekpzkY72W",
        "outputId": "d99f2926-7c55-4ab2-e5c1-59fb6e00adc5"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # features\n",
        "y = iris.target  # target labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5kDmttd1fG0"
      },
      "source": [
        "# The end"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
