{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec9a2b3",
   "metadata": {},
   "source": [
    "# Lab instruction: linearRegression\n",
    "\n",
    "### First exercise\n",
    "\n",
    "##### Objective\n",
    "To understand the effects of regularization in linear regression and the bias-variance tradeoff\n",
    "\n",
    "\n",
    "##### Instructions\n",
    "\n",
    "1. **Dataset**\n",
    "   In this notebook it is provided a function that generates points that come from  polynomial. The parameters for the polynomial  $$x^4 - 10x^3 + 35x^2 - 50x + 2$$\n",
    "   Afterwards, we generate samples should be contaminated with zero mean white Gaussian noise with customizable standard deviation. This funcrion has as input the range of variable x. Hint: Use the polyval function of numpy.\n",
    "   Generate 30 points in the range of x = [1.0, 4.25].\n",
    "\n",
    "2. **Make the design matrix**\n",
    "   We are implementing a polynomial regression. This is non-linear. However, as we saw in the lecture we can consider that each non-linear component of the polynomial. As a hint. To build your design matrix use the  PolynomialFeatures routine from  sklearn.preprocessing. You need to add an extra dimension to your x array by x.reshape(-1, 1). \n",
    "\n",
    "\n",
    "3. **Learning algorithms**\n",
    "   Use scipy's LinearRegression, Lasso, ridge_regression from sklearn.linear_model to estimate the parameters of the model that generates the data. Make a regression of 15th degree order. For each model \n",
    "   * For LASSO use , tol=0, max_iter=10000.\n",
    "   * For Ridge use the as solver \"cholesky\". What happens if you set the alpha value to zero?\n",
    "\n",
    "4. **Evaluate results**\n",
    "   How are each of the regressions? Can you say something about their complexity with respect to the data? make a plot which displays the coefficients of each model? Which one has more non-zero elements? try different values of alpha for the LASSO and Ridge regressions.\n",
    "   \n",
    "   Reflect upon the following questions\n",
    "   * Why does Ridge regression does not perform parameter selection and LASSO does?\n",
    "   * What is the difference between Ridge regression and LASSO?\n",
    "   * What is the difference between Ridge regression with alpha set to zero and regular linear regression?\n",
    "   * What happens with the models  if we have more samples (e.g. 300 samples)?\n",
    "   * What happens if you reduce the compolexity of the model? For example, we know that the data comes from a 4th order polynomial, but our model is order 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d82b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_noisy_polynomial_data(coeffs, num_points=100, noise_std=1.0, x_range=(-10, 10)):\n",
    "    \"\"\"\n",
    "    Generates data points from a polynomial of arbitrary degree with Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    - coeffs: list or array of coefficients [a_n, ..., a_1, a_0] for a_n*x^n + ... + a_1*x + a_0\n",
    "    - num_points: number of data points to generate\n",
    "    - noise_std: standard deviation of Gaussian noise\n",
    "    - x_range: tuple (min_x, max_x) defining the range of x values\n",
    "\n",
    "    Returns:\n",
    "    - x: numpy array of x values\n",
    "    - y_noisy: numpy array of noisy y values\n",
    "\n",
    "    # During the preparation of this work, Luis A.Zavala Mondragon used MS. Copilot  in order to \n",
    "    # generate this function.  After using this tool/service, Luis A.Zavala Mondrago evaluated \n",
    "    # the validity of the tool’s outputs, including the sources that generative AI tools have used,\n",
    "    # and edited the content as needed. As a consequence, Luis A.Zavala Mondragon takes full \n",
    "    # responsibility for the content of their work.\n",
    "    \"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    y = np.polyval(coeffs, x)\n",
    "    noise = np.random.normal(0, noise_std, size=num_points)\n",
    "    y_noisy = y + noise\n",
    "    return x, y_noisy\n",
    "\n",
    "\n",
    "def plot_regression(x, y_inp, y_true,  y_est=[], showEstimate=False, estimateName=\"\"):\n",
    "    \"\"\"\n",
    "    Generates plots to visualize the generated points, true line and estimated curve\n",
    "\n",
    "    Parameters:\n",
    "    - x: list or array of coefficients [a_n, ..., a_1, a_0] for a_n*x^n + ... + a_1*x + a_0\n",
    "    - y_inp: The noisy y values\n",
    "    - y_true: The noiseless y values\n",
    "    - y_est: The estimated values of y with one of the regression methods\n",
    "    - showEstimate: True when you want to display y_est\n",
    "    - estimateName: Name for display of the line y_est\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    # During the preparation of this work, Luis A.Zavala Mondragon used MS. Copilot  in order to \n",
    "    # generate part of this function.  After using this tool/service, Luis A.Zavala Mondrago evaluated \n",
    "    # the validity of the tool’s outputs, including the sources that generative AI tools have used,\n",
    "    # and edited the content as needed. As a consequence, Luis A.Zavala Mondragon takes full \n",
    "    # responsibility for the content of their work.\n",
    "    \"\"\"\n",
    "    # Plotting\n",
    "    plt.scatter(x, y_inp, label='Noisy Data', alpha=0.6)\n",
    "    plt.plot(x, y_true, color='red', label='True Polynomial')\n",
    "    if showEstimate:\n",
    "        plt.plot(x, y_est, color=\"green\", label=estimateName)\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Noisy Polynomial Data (4$^\\mathrm{th}$ Degree)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# These are the coefficientts of the polynomial\n",
    "coeffs = [1, -10, 35,-50, 24] \n",
    "x, y = generate_noisy_polynomial_data(coeffs, num_points=30, noise_std=1.5, x_range=(1.0, 4.25))\n",
    "y_true = np.polyval(coeffs, x)\n",
    "\n",
    "plot_regression(x, y_inp=y, y_true=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Add your code here to implement the regressions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1879d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
