<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Instructionmodule4 - MSc AI & Engineering Systems Notes">
    <title>Instructionmodule4 - MSc AIES</title>
    <link rel="icon" href="../../../../../../assets/svg/favicon.svg" type="image/svg+xml">
    <link rel="stylesheet" href="../../../../../../styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            // Initialize KaTeX
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
            
            // Initialize Prism.js syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
            
            // Handle theme switching for Prism.js
            function updatePrismTheme() {
                const isDark = document.body.classList.contains('dark-theme');
                const prismLink = document.querySelector('link[href*="prism"]');
                if (prismLink) {
                    if (isDark) {
                        prismLink.href = 'https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css';
                    } else {
                        prismLink.href = 'https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css';
                    }
                }
            }
            
            // Check initial theme
            updatePrismTheme();
            
            // Listen for theme changes
            const observer = new MutationObserver(function(mutations) {
                mutations.forEach(function(mutation) {
                    if (mutation.type === 'attributes' && mutation.attributeName === 'class') {
                        updatePrismTheme();
                    }
                });
            });
            
            observer.observe(document.body, {
                attributes: true,
                attributeFilter: ['class']
            });
        });
    </script>
    <style>
        .note-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
        }
        .note-header {
            text-align: center;
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 2px solid var(--border-color);
        }
        .note-title {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            color: var(--primary-color);
        }
        .back-link {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.4rem 0.8rem;
            color: var(--text-color);
            text-decoration: none;
            font-size: 0.9rem;
            font-weight: 400;
            background: transparent;
            border: 1px solid var(--border-color);
            border-radius: 4px;
            transition: all 0.2s ease;
        }
        .back-link:hover {
            color: var(--primary-color);
            border-color: var(--primary-color);
        }
        .nav-left {
            display: flex;
            flex-direction: column;
            gap: 0.75rem;
        }
        
        .breadcrumb-section {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }
        .folder-path {
             font-size: 0.8rem;
             color: var(--text-color-secondary);
             opacity: 0.7;
         }
         .current-page-title {
             font-size: 1.3rem;
             font-weight: 700;
             color: var(--text-color);
             margin: 0;
             line-height: 1.3;
             letter-spacing: -0.01em;
         }
        .note-content {
            line-height: 1.8;
            color: var(--text-color);
        }
        .note-content h1, .note-content h2, .note-content h3 {
            color: var(--primary-color);
            margin-top: 2rem;
            margin-bottom: 1rem;
        }
        .note-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 1rem 0;
            display: block;
        }
        /* Preserve centered images from markdown HTML */
        .note-content div[style*="text-align: center"] {
            text-align: center !important;
        }
        .note-content div[style*="text-align: center"] img {
            margin: 1rem auto;
        }
        .note-content pre {
            background: var(--code-bg);
            padding: 1rem;
            border-radius: 5px;
            overflow-x: auto;
        }
        .note-content code {
            background: var(--code-bg);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .note-content blockquote {
            border-left: 4px solid var(--primary-color);
            padding-left: 1rem;
            margin: 1rem 0;
            font-style: italic;
            opacity: 0.9;
        }
        .note-content ul, .note-content ol {
            padding-left: 2rem;
            margin: 1rem 0;
        }
        .note-content ul {
            list-style-type: disc;
        }
        .note-content li {
            margin-bottom: 0.2rem;
            display: list-item;
            list-style-position: outside;
        }
        /* Reduce spacing for nested lists */
        .note-content li ul, .note-content li ol {
            margin: 0.3rem 0;
            padding-left: 1.5rem;
        }
        .note-content ul li::marker {
            color: var(--primary-color);
            font-size: 1.2em;
        }
        
        /* Q&A Styles */
        .qa-item {
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
            background: var(--bg-color);
        }
        
        .qa-question {
            padding: 1rem;
            background: var(--secondary-bg, #f8f9fa);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 0.75rem;
            transition: background-color 0.2s ease;
            user-select: none;
        }
        
        .qa-question:hover {
            background: var(--hover-bg, #e9ecef);
        }
        
        .qa-icon {
            font-size: 0.8rem;
            transition: transform 0.2s ease;
            color: var(--primary-color);
            font-weight: bold;
        }
        
        .qa-question.active .qa-icon {
            transform: rotate(90deg);
        }
        
        .qa-text {
            font-weight: 600;
            color: var(--text-color);
            flex: 1;
        }
        
        .qa-answer {
            padding: 1rem;
            background: var(--bg-color);
            border-top: 1px solid var(--border-color);
            display: none;
            color: var(--text-color);
            line-height: 1.6;
        }
        
        .qa-answer.active {
            display: block;
        }
        
        /* Mobile responsive styles */
        @media (max-width: 768px) {
            .note-container {
                padding: 0.5rem;
            }
            .navigation-header {
                padding: 1rem;
                gap: 1rem;
                margin-bottom: 1.5rem;
            }
            .nav-left {
                gap: 1rem;
            }
            .breadcrumb-section {
                flex-direction: column;
                align-items: flex-start;
                gap: 0.5rem;
            }
            .current-page-title {
                font-size: 1.1rem;
                text-align: left;
            }
            .back-link {
                font-size: 0.9rem;
                padding: 0.6rem 1rem;
                min-height: 40px;
                display: flex;
                align-items: center;
                justify-content: center;
                border-radius: 6px;
                background: var(--border-color);
                color: var(--text-color) !important;
                text-decoration: none;
                border: 1px solid var(--border-color);
                transition: all 0.2s ease;
                flex-shrink: 0;
            }
            .back-link:hover {
                background: var(--primary-color);
                color: white !important;
                border-color: var(--primary-color);
            }
            .folder-path {
                font-size: 0.85rem;
                color: var(--text-color-secondary);
                opacity: 0.8;
                flex: 1;
                min-width: 0;
            }
            .note-navigation {
                width: 100%;
                display: flex;
                align-items: center;
                gap: 0.75rem;
                flex-wrap: nowrap;
            }
            .nav-button {
                font-size: 1.2rem;
                padding: 0.5rem;
                min-height: 40px;
                min-width: 40px;
                border-radius: 50%;
                display: flex;
                align-items: center;
                justify-content: center;
                flex-shrink: 0;
            }
            .nav-button span:not(.nav-icon) {
                display: none;
            }
            .nav-progress {
                flex: 1;
                font-size: 0.8rem;
                text-align: center;
                color: var(--text-color-secondary);
            }
            .note-header {
                margin-bottom: 1rem;
                padding-bottom: 1.5rem;
            }
            .note-title {
                font-size: 2rem;
            }
            .note-content h1 {
                font-size: 1.8rem;
            }
            .note-content h2 {
                font-size: 1.5rem;
            }
            .note-content h3 {
                font-size: 1.3rem;
            }
            .note-content pre {
                padding: 0.75rem;
                font-size: 0.9rem;
            }
            .note-content ul, .note-content ol {
                padding-left: 1.5rem;
            }
        }
        
        @media (max-width: 480px) {
             .note-container {
                 padding: 0.5rem;
             }
             .note-header {
                 margin-bottom: 1.15rem;
                 padding-bottom: 0.75rem;
             }
             .back-link {
                 padding: 0.25rem 0.5rem;
                 font-size: 0.85rem;
             }
         }
        
        /* Navigation header styles */
         .navigation-header {
             display: flex;
             flex-direction: column;
             gap: 1.25rem;
             margin-bottom: 2rem;
             padding: 1.25rem;
             background: transparent;
             border-radius: 0;
             border: none;
             border-bottom: 1px solid var(--border-color);
         }
         
         /* Navigation buttons styles */
         .note-navigation {
             display: flex;
             justify-content: center;
             align-items: center;
             gap: 1rem;
         }
         
         .nav-button {
             display: inline-flex;
             align-items: center;
             justify-content: center;
             padding: 0.6rem;
             background: transparent;
             border: 1px solid var(--border-color);
             border-radius: 4px;
             color: var(--text-color);
             text-decoration: none;
             font-size: 1rem;
             font-weight: 400;
             transition: all 0.2s ease;
             width: 36px;
             height: 36px;
         }
         
         .nav-button:hover {
             color: var(--primary-color);
             border-color: var(--primary-color);
         }
         
         .nav-button:disabled {
             opacity: 0.5;
             cursor: not-allowed;
             pointer-events: none;
         }
         
         .nav-button.prev {
             justify-content: flex-start;
         }
         
         .nav-button.next {
             justify-content: flex-end;
         }
         
         .nav-button-text {
             display: flex;
             flex-direction: column;
             align-items: inherit;
         }
         
         .nav-button-label {
             font-size: 0.75rem;
             opacity: 0.7;
             margin-bottom: 0.2rem;
         }
         
         .nav-button-title {
             font-weight: 500;
             max-width: 200px;
             overflow: hidden;
             text-overflow: ellipsis;
             white-space: nowrap;
         }
         
         .nav-progress {
             display: flex;
             flex-direction: column;
             align-items: center;
             gap: 0.5rem;
             flex: 1;
             margin: 0 1rem;
         }
         
         .nav-progress-text {
             font-size: 0.8rem;
             font-weight: 400;
             color: var(--text-color);
             opacity: 0.7;
             text-align: center;
         }
         
         .nav-progress-bar {
             width: 100%;
             max-width: 200px;
             height: 4px;
             background: #f0f0f0;
             border-radius: 2px;
             overflow: hidden;
         }
         
         .nav-progress-fill {
             height: 100%;
             background: #007acc;
             transition: width 0.3s ease;
         }
         
         @media (max-width: 768px) {
              .note-navigation {
                  flex-direction: row;
                  gap: 0.75rem;
                  align-items: center;
              }
              
              .nav-button {
                  width: 40px;
                  min-width: 40px;
                  height: 40px;
                  border-radius: 50%;
                  padding: 0.5rem;
                  font-size: 1.2rem;
              }
              
              .nav-progress {
                  flex: 1;
                  margin: 0;
              }
              
              .nav-button-title {
                  max-width: none;
              }
          }
          
          /* Jupyter Notebook Styles */
          .code-cell {
              margin: 1.5rem 0;
              border: 1px solid var(--border-color);
              border-radius: 8px;
              overflow: hidden;
              background: var(--bg-color);
          }
          
          .markdown-cell {
              margin: 1.5rem 0;
              padding: 1rem;
              background: var(--bg-color);
              border-radius: 8px;
          }
          
          .input-area {
              background: var(--code-bg, #f8f9fa);
              border-bottom: 1px solid var(--border-color);
          }
          
          .input-area pre {
              margin: 0;
              padding: 1rem;
              background: transparent;
              border-radius: 0;
              overflow-x: auto;
          }
          
          .input-area code {
              background: transparent;
              padding: 0;
              border-radius: 0;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.9rem;
              line-height: 1.4;
          }
          
          .output-area {
              background: var(--bg-color);
              padding: 1rem;
          }
          
          .output-stream {
              background: var(--secondary-bg, #f1f3f4);
              padding: 0.75rem;
              margin: 0.5rem 0;
              border-radius: 4px;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.85rem;
              line-height: 1.4;
              color: var(--text-color);
              border-left: 3px solid var(--primary-color);
          }
          
          .output-result {
              background: var(--bg-color);
              padding: 0.75rem;
              margin: 0.5rem 0;
              border-radius: 4px;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.85rem;
              line-height: 1.4;
              color: var(--text-color);
              border: 1px solid var(--border-color);
          }
          
          .output-image {
              text-align: center;
              margin: 1rem 0;
              padding: 0.5rem;
              background: var(--bg-color);
              border-radius: 4px;
          }
          
          .output-image img {
              max-width: 100%;
              height: auto;
              border-radius: 4px;
              box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
              margin: 0;
          }
          
          .output-html {
              margin: 0.5rem 0;
              padding: 0.75rem;
              background: var(--bg-color);
              border-radius: 4px;
              border: 1px solid var(--border-color);
              overflow-x: auto;
          }
          
          .output-html table {
              width: 100%;
              border-collapse: collapse;
              margin: 0;
          }
          
          .output-html th,
          .output-html td {
              padding: 0.5rem;
              border: 1px solid var(--border-color);
              text-align: left;
          }
          
          .output-html th {
              background: var(--secondary-bg, #f8f9fa);
              font-weight: 600;
          }
          
          .output-error {
              background: #fef2f2;
              border: 1px solid #fecaca;
              border-radius: 4px;
              margin: 0.5rem 0;
              padding: 0.75rem;
          }
          
          .error-name {
              color: #dc2626;
              font-weight: 600;
              margin: 0 0 0.5rem 0;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.9rem;
          }
          
          .error-traceback {
              color: #7f1d1d;
              margin: 0;
              font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
              font-size: 0.8rem;
              line-height: 1.4;
              background: #fef7f7;
              padding: 0.5rem;
              border-radius: 3px;
              overflow-x: auto;
          }
          
          /* Dark theme adjustments for error outputs */
          .dark-theme .output-error {
              background: #2d1b1b;
              border-color: #7f1d1d;
          }
          
          .dark-theme .error-name {
              color: #f87171;
          }
          
          .dark-theme .error-traceback {
              color: #fca5a5;
              background: #1f1515;
          }
          
          /* Dark mode adjustments for Jupyter cells */
          @media (prefers-color-scheme: dark) {
              .input-area {
                  background: var(--code-bg, #2d3748);
              }
              
              .output-stream {
                  background: var(--secondary-bg, #4a5568);
              }
          }
    </style>
</head>
<body>
    <div class="menu-overlay"></div>
    <header>
        <nav class="nav-container">
            <div class="hamburger" id="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
            <ul class="nav-links" id="navLinks">
                <li><a href="../../../../../../index.html">About</a></li>
                <li><a href="../../../../../../journey.html">My Life</a></li>
                <li><a href="../../../../../../projects.html">Projects</a></li>
                <li><a href="../../../../../../blogs.html">Blog</a></li>
                <li><a href="../../../../../../notes.html">MSc AIES</a></li>
                <li><a href="https://dailyclips.es/" target="_blank">Daily Clips</a></li>
                <li><a href="https://alvaromenendez.es/ufc-predictions/" target="_blank">UFC Predictions</a></li>
                <li><a href="../../../../../../acknowledgments.html">Acknowledgments</a></li>
                <li><a href="../../../../../../assets/Alvaro_Menendez_CV.pdf" download>CV</a></li>
            </ul>
            <div class="nav-right">
                <div class="social-links">
                    <a href="https://github.com/DKeAlvaro" target="_blank" aria-label="GitHub">
                        <img src="../../../../../../assets/svg/github.svg" alt="GitHub" class="social-icon">
                    </a>
                    <a href="https://www.linkedin.com/in/alvaromenendezros" target="_blank" aria-label="LinkedIn">
                        <img src="../../../../../../assets/svg/linkedin.svg" alt="LinkedIn" class="social-icon">
                    </a>
                    <a href="mailto:alvaro.mrgr@gmail.com" aria-label="Email">
                        <img src="../../../../../../assets/svg/gmail.svg" alt="Email" class="social-icon">
                    </a>
                </div>
                <div class="theme-toggle" id="themeToggle" aria-label="Toggle theme" role="button" tabindex="0">
                    <img src="../../../../../../assets/svg/moon.svg" alt="Toggle theme" class="theme-icon" id="themeIcon">
                </div>
            </div>
        </nav>
    </header>

    <div class="page-container">
        <div class="content">
            <div class="note-container">
                <div class="navigation-header">
                    <div class="nav-left">
                        <div class="breadcrumb-section">
                            <a href="../../../../../../notes.html" class="back-link">
                                <span>←</span>
                                <span>Back to Notes</span>
                            </a>
                            <span class="folder-path">Year_1 / Q1 / Data_Analysis_&_Learning_Methods / Instructions</span>
                        </div>
                        <div class="current-page-title" id="currentPageTitle"></div>
                    </div>
                    
                    <div class="note-navigation" id="noteNavigation">
                        <a href="#" class="nav-button prev" id="prevButton" style="visibility: hidden;" title="Previous note">
                            <span class="nav-icon">←</span>
                        </a>
                        
                        <div class="nav-progress">
                            <div class="nav-progress-text" id="progressText">Loading...</div>
                            <div class="nav-progress-bar">
                                <div class="nav-progress-fill" id="progressFill" style="width: 0%;"></div>
                            </div>
                        </div>
                        
                        <a href="#" class="nav-button next" id="nextButton" style="visibility: hidden;" title="Next note">
                            <span class="nav-icon">→</span>
                        </a>
                    </div>
                </div>
                
                <div class="note-content">
                    <div class="markdown-cell">
<h1>Instructions 4</h1>
</div>
<div class="markdown-cell">
<h2>Introduction</h2>
<p>In this assignment you will perform unsupervised learning through 4 different clustering algorithms, you will carry on this task with Iris dataset and toy datasets, to compare between these different approaches, you will employ different metrics, such as accuracy and Xie-beni index. By the end of this tutorial you are expected to achieve the following learning goals:</p>
<h3>Learning goals:</h3>
<p>After this tutorial you can:
- perform Hierarchical clustering with different linkage settings;
- Perform K-means clustering with different clusters;
- perform FCM clustering with different clusters;
- Visualize the inner workings of FCM using toy dataset
- perform GK clustering with different clusters;</p>
</div>
<div class="markdown-cell">
<p>libraries needed for this assignment:</p>
<ul>
<li>sklearn (install: !pip install scikit-learn)</li>
<li>skfuzzy (install: !pip install scikit-fuzzy)</li>
<li>scipy (install: !pip install scipy)</li>
<li>pandas (install: !pip install pandas)</li>
<li>numpy (install: !pip install numpy)</li>
<li>matplotlib (install: !pip install matplotlib)</li>
</ul>
</div>
<div class="markdown-cell">
<p><code>Please import the following libraries:</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree
from scipy.optimize import linear_sum_assignment 
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score
import skfuzzy as fuzz
from scipy.linalg import norm
from sklearn.metrics import confusion_matrix


</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Loading the Iris dataset</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# loading and normalizing Iris dataset
iris = load_iris()
iris_data = iris.data
true_labels = iris.target  # True labels

# Normalize the data using z-score
scaler = StandardScaler()
iris_data_normalized = scaler.fit_transform(iris_data)

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>Part 1: Hierarchical Clustering</h2>
</div>
<div class="markdown-cell">
<p>For this taks you will carry on <code>Agglomerative clustering</code> also known as Agglomerative Nesting (AGNES); it is a type of hierarchical clustering, specifically a bottom-up approach. It starts with each data point as its own individual cluster and then iteratively merges the closest clusters until all points belong to a single cluster, or until a specified number of clusters is reached.</p>
<p>There are several methods to measure similarity between two points, for this exercise we will use the Euclidean distance. Which is measured in different ways: Single linkage, Complete linkage, Average linkage, Ward linkage.</p>
<ul>
<li>
<p>Load and Normalize: Load the Iris dataset and apply z-score normalization to the features.</p>
</li>
<li>
<p>Agglomerative Clustering: Perform agglomerative clustering using single linkage with an initial distance threshold of 0.0.</p>
</li>
<li>
<p>Determine Optimal Threshold: Experiment with various distance thresholds (e.g., 0.0, 0.5, 1.0) and identify the optimal threshold based on the accuracy metric of the clusters formed.</p>
</li>
<li>
<p>Visualize Dendrograms: Create two subplots: A truncated dendrogram showing the top 3 levels with the chosen threshold marked, and a full dendrogram displaying all levels.</p>
</li>
<li>
<p>Scatter Plot: Plot a scatter diagram using two selected features from the Iris dataset, coloring points by the optimal clusters and the actual classes.</p>
</li>
<li>
<p>Find the optimal distance threshold when using complete and average linkage.</p>
</li>
<li>
<p>Discussion: Reflect on the optimal distance threshold, the number of clusters formed, and how well the clusters correspond to the actual classes.</p>
</li>
</ul>
</div>
<div class="markdown-cell">
<h3>Exercise 1.1: Single Linkage</h3>
</div>
<div class="markdown-cell">
<p><code>Perform agglomerative clustering with single linkage and zero distance threshold</code></p>
<p>When performing agglomerative clustering with single linkage and a zero distance threshold, your clustering function should return the following:</p>
<ol>
<li>
<p><strong>Linkage Matrix (<code>Z</code>)</strong>: The hierarchical clustering encoded as a linkage matrix, which can be obtained using the <code>linkage</code> function from <code>scipy.cluster.hierarchy</code>.</p>
</li>
<li>
<p><strong>Cluster Labels (<code>clusters</code>)</strong>: An array containing the cluster labels for each data point, based on the specified distance threshold. You can get this by applying the <code>cut_tree</code> function to the linkage matrix with the height parameter set to the distance threshold.</p>
</li>
<li>
<p><strong>Distance Threshold (<code>distance_threshold</code>)</strong>: The threshold value used for forming clusters. This is essential for plotting and should be set to zero, as specified.</p>
</li>
</ol>
<p>Returning these three variables will allow the provided plotting function to display the dendrograms accurately.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# write your Clustering function here

# return Z, clusters, distance_threshold = perform_clustering()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Use the following plotting function to visualize your dendrograms</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">

# Plotting function
def plot_dendrogram(Z, clusters, distance_threshold=0.0):
    # Create a figure with two subplots
    fig, axs = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the truncated dendrogram (top 3 levels)
    dendrogram(Z, 
               truncate_mode='level',  
               p=3,                    
               labels=clusters,        
               leaf_rotation=0,        
               leaf_font_size=10,      
               ax=axs[0])              
    axs[0].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[0].set_title("Dendrogram (Top 3 Levels)")
    axs[0].set_xlabel("Sample Index or Cluster")
    axs[0].set_ylabel("Distance")

    # Plot the full dendrogram (all levels)
    dendrogram(Z, 
               truncate_mode=None,  
               labels=clusters,     
               leaf_rotation=0,     
               leaf_font_size=10,   
               ax=axs[1],           
               show_leaf_counts=False)  
    axs[1].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[1].set_title("Dendrogram (Full)")
    axs[1].set_xlabel("")  
    axs[1].set_ylabel("Distance")
    axs[1].set_xticks([])

    # Set the main title for the entire figure
    plt.suptitle("Dendrograms Single Linkage Zero Distance Threshold")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()



# Use the results for plotting
plot_dendrogram(Z, clusters, distance_threshold)

</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Using the following accuracy function to answer the following questions</code>
- What is the optimal distance threshold yielding the best accuracy?
- What is the distance threshold yielding 3 clusters?</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# set distance threshold, replace 0 value with your value
distance_threshold = 0.05

# Define the function to calculate accuracy
def calculate_accuracy(clusters, true_labels):
    cm = confusion_matrix(true_labels, clusters)
    best_accuracy = np.trace(cm) / np.sum(cm)
    return best_accuracy

def manual_threshold_clustering(Z, true_labels, distance_threshold):
    # Create clusters using the specified distance threshold
    clusters = cut_tree(Z, height=distance_threshold).flatten()
    
    # Count the number of unique clusters
    num_clusters = len(np.unique(clusters))
    
    # Calculate accuracy
    best_accuracy = calculate_accuracy(clusters, true_labels)
    
    # Print results
    print(f"Distance Threshold: {distance_threshold}")
    print(f"Number of clusters formed: {num_clusters}")
    print(f"Accuracy achieved: {best_accuracy:.4f}")
    
    # Plot the dendrogram
    plot_dendrogram(Z, clusters, distance_threshold)
    
    return best_accuracy   # <-- return this value


# Dendrogram plotting function
def plot_dendrogram(Z, clusters, distance_threshold):
    # Create a figure with two subplots
    fig, axs = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the truncated dendrogram (top 3 levels)
    dendrogram(Z, 
               truncate_mode='level',  
               p=3,                    
               leaf_rotation=0,        
               leaf_font_size=10,      
               ax=axs[0])              
    axs[0].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[0].set_title("Dendrogram (Top 3 Levels)")
    axs[0].set_xlabel("Sample Index or Cluster")
    axs[0].set_ylabel("Distance")

    # Plot the full dendrogram (all levels)
    dendrogram(Z, 
               truncate_mode=None,  
               labels=clusters,     
               leaf_rotation=0,     
               leaf_font_size=10,   
               ax=axs[1],           
               show_leaf_counts=False)  
    axs[1].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[1].set_title("Dendrogram (Full)")
    axs[1].set_xlabel("")  
    axs[1].set_ylabel("Distance")
    axs[1].set_xticks([])

    # Set the main title for the entire figure
    plt.suptitle("Dendrograms with Distance Threshold")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# Ask for manual input of the distance threshold
manual_threshold_clustering(Z, true_labels, distance_threshold)

</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Based on your experiments, set the best_threshold value as the distance threshold resulting in 3 clusters to visualize Iris (Petal width and Petal length)</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
best_threshold = 1.2
best_accuracy = manual_threshold_clustering(Z, true_labels, distance_threshold)


# Perform agglomerative clustering with single linkage
Z = linkage(iris_data_normalized, method='single')


# Find the number of clusters formed with the best threshold
best_clusters = cut_tree(Z, height=best_threshold).flatten()
num_clusters = len(np.unique(best_clusters))

print(f"Best Distance Threshold: {best_threshold}")
print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Number of clusters formed: {num_clusters}")

# Define marker shapes for each cluster
markers = ['o', 's', 'D', 'X']  # Circle, Square, Diamond, X
colors = ['r', 'g', 'b']  # Colors for true labels (0, 1, 2)

# Create a figure with two subplots
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Plot clusters with different shapes
for cluster in range(num_clusters):
    cluster_points = iris_data_normalized[best_clusters == cluster]
    
    # Use the third and fourth features (Petal length and Petal width)
    axs[0].scatter(
        cluster_points[:, 2],  # Petal length
        cluster_points[:, 3],  # Petal width
        marker=markers[cluster % len(markers)],  # Different shape for each cluster
        label=f'Cluster {cluster}'
    )

axs[0].set_title('Clusters Visualization')
axs[0].set_xlabel('Petal Length (standardized)')
axs[0].set_ylabel('Petal Width (standardized)')
axs[0].legend()
axs[0].grid()

# Plot true labels with different colors
for label in np.unique(true_labels):
    label_points = iris_data_normalized[true_labels == label]
    
    axs[1].scatter(
        label_points[:, 2],  # Petal length
        label_points[:, 3],  # Petal width
        label=f'Class {label}',
        alpha=0.5  # Slight transparency for better visualization
    )

axs[1].set_title('True Labels Visualization')
axs[1].set_xlabel('Petal Length (standardized)')
axs[1].set_ylabel('Petal Width (standardized)')
axs[1].legend()
axs[1].grid()

# Set the main title for the entire figure
plt.suptitle("Single Linkage Optimal Distance Threshold")

plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to reserve space for the main title
plt.show()

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>Exercise 1.2: Complete linkage</h3>
<p>Perform agglomerative clustering using complete linkage. In a similar manner find the optimal distance threshold.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Clustering function



# return Z, clusters, distance_threshold = perform_clustering()

</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Visualize your dendograms and experiment with the distance threshold</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">

# set distance threshold, replace 0 value with your value
distance_threshold = 0.05

# Define the function to calculate accuracy
def calculate_accuracy(clusters, true_labels):
    cm = confusion_matrix(true_labels, clusters)
    accuracy = np.trace(cm) / np.sum(cm)
    return accuracy

# Function to perform clustering based on a manually entered threshold
def manual_threshold_clustering(Z, true_labels, distance_threshold):
    # Create clusters using the specified distance threshold
    clusters = cut_tree(Z, height=distance_threshold).flatten()
    
    # Count the number of unique clusters
    num_clusters = len(np.unique(clusters))
    
    # Calculate accuracy
    accuracy = calculate_accuracy(clusters, true_labels)
    
    # Print results
    print(f"Distance Threshold: {distance_threshold}")
    print(f"Number of clusters formed: {num_clusters}")
    print(f"Accuracy achieved: {accuracy:.4f}")
    
    # Plot the dendrogram
    plot_dendrogram(Z, clusters, distance_threshold)

# Dendrogram plotting function
def plot_dendrogram(Z, clusters, distance_threshold):
    # Create a figure with two subplots
    fig, axs = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the truncated dendrogram (top 3 levels)
    dendrogram(Z, 
               truncate_mode='level',  
               p=3,                    
               leaf_rotation=0,        
               leaf_font_size=10,      
               ax=axs[0])              
    axs[0].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[0].set_title("Dendrogram (Top 3 Levels)")
    axs[0].set_xlabel("Sample Index or Cluster")
    axs[0].set_ylabel("Distance")

    # Plot the full dendrogram (all levels)
    dendrogram(Z, 
               truncate_mode=None,  
               labels=clusters,     
               leaf_rotation=0,     
               leaf_font_size=10,   
               ax=axs[1],           
               show_leaf_counts=False)  
    axs[1].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[1].set_title("Dendrogram (Full)")
    axs[1].set_xlabel("")  
    axs[1].set_ylabel("Distance")
    axs[1].set_xticks([])

    # Set the main title for the entire figure
    plt.suptitle("Dendrograms with Distance Threshold")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# Ask for manual input of the distance threshold
manual_threshold_clustering(Z, true_labels, distance_threshold)
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Visuliaze Iris dataset using your distance threshold</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
best_threshold = 1.30

# Perform agglomerative clustering with single linkage
Z = linkage(iris_data_normalized, method='complete')


# Find the number of clusters formed with the best threshold
best_clusters = cut_tree(Z, height=best_threshold).flatten()
num_clusters = len(np.unique(best_clusters))

print(f"Best Distance Threshold: {best_threshold}")
print(f"Number of clusters formed: {num_clusters}")

# Define marker shapes for each cluster
markers = ['o', 's', 'D', 'X']  # Circle, Square, Diamond, X
colors = ['r', 'g', 'b']  # Colors for true labels (0, 1, 2)

# Create a figure with two subplots
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Plot clusters with different shapes
for cluster in range(num_clusters):
    cluster_points = iris_data_normalized[best_clusters == cluster]
    
    # Use the third and fourth features (Petal length and Petal width)
    axs[0].scatter(
        cluster_points[:, 2],  # Petal length
        cluster_points[:, 3],  # Petal width
        marker=markers[cluster % len(markers)],  # Different shape for each cluster
        label=f'Cluster {cluster}'
    )

axs[0].set_title('Clusters Visualization')
axs[0].set_xlabel('Petal Length (standardized)')
axs[0].set_ylabel('Petal Width (standardized)')
axs[0].legend()
axs[0].grid()

# Plot true labels with different colors
for label in np.unique(true_labels):
    label_points = iris_data_normalized[true_labels == label]
    
    axs[1].scatter(
        label_points[:, 2],  # Petal length
        label_points[:, 3],  # Petal width
        label=f'Class {label}',
        alpha=0.5  # Slight transparency for better visualization
    )

axs[1].set_title('True Labels Visualization')
axs[1].set_xlabel('Petal Length (standardized)')
axs[1].set_ylabel('Petal Width (standardized)')
axs[1].legend()
axs[1].grid()

# Set the main title for the entire figure
plt.suptitle("Single Linkage Optimal Distance Threshold")

plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to reserve space for the main title
plt.show()

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>Exercise 1.3: Average linkage</h3>
</div>
<div class="markdown-cell">
<p><code>Perform as the previous exercise but this time use an average linkage</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# create average clustering fuction


# return Z, clusters, distance_threshold = perform_clustering()


</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Visualize your dendograms and experiment with the distance threshold</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">

# set distance threshold, replace 0 value with your value
distance_threshold = 0.5

# Define the function to calculate accuracy
def calculate_accuracy(clusters, true_labels):
    cm = confusion_matrix(true_labels, clusters)
    accuracy = np.trace(cm) / np.sum(cm)
    return accuracy

# Function to perform clustering based on a manually entered threshold
def manual_threshold_clustering(Z, true_labels, distance_threshold):
    # Create clusters using the specified distance threshold
    clusters = cut_tree(Z, height=distance_threshold).flatten()
    
    # Count the number of unique clusters
    num_clusters = len(np.unique(clusters))
    
    # Calculate accuracy
    accuracy = calculate_accuracy(clusters, true_labels)
    
    # Print results
    print(f"Distance Threshold: {distance_threshold}")
    print(f"Number of clusters formed: {num_clusters}")
    print(f"Accuracy achieved: {accuracy:.4f}")
    
    # Plot the dendrogram
    plot_dendrogram(Z, clusters, distance_threshold)

# Dendrogram plotting function
def plot_dendrogram(Z, clusters, distance_threshold):
    # Create a figure with two subplots
    fig, axs = plt.subplots(1, 2, figsize=(15, 6))

    # Plot the truncated dendrogram (top 3 levels)
    dendrogram(Z, 
               truncate_mode='level',  
               p=3,                    
               leaf_rotation=0,        
               leaf_font_size=10,      
               ax=axs[0])              
    axs[0].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[0].set_title("Dendrogram (Top 3 Levels)")
    axs[0].set_xlabel("Sample Index or Cluster")
    axs[0].set_ylabel("Distance")

    # Plot the full dendrogram (all levels)
    dendrogram(Z, 
               truncate_mode=None,  
               labels=clusters,     
               leaf_rotation=0,     
               leaf_font_size=10,   
               ax=axs[1],           
               show_leaf_counts=False)  
    axs[1].axhline(y=distance_threshold, color='r', linestyle='--')
    axs[1].set_title("Dendrogram (Full)")
    axs[1].set_xlabel("")  
    axs[1].set_ylabel("Distance")
    axs[1].set_xticks([])

    # Set the main title for the entire figure
    plt.suptitle("Dendrograms with Distance Threshold")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# Ask for manual input of the distance threshold
manual_threshold_clustering(Z, true_labels, distance_threshold)
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>Visualize iris dataset with 3 cluster and your distance threshold</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
best_threshold = 1.30



# Perform agglomerative clustering with single linkage
Z = linkage(iris_data_normalized, method='average')


# Find the number of clusters formed with the best threshold
best_clusters = cut_tree(Z, height=best_threshold).flatten()
num_clusters = len(np.unique(best_clusters))

print(f"Best Distance Threshold: {best_threshold}")
print(f"Best Accuracy: {best_accuracy:.4f}")
print(f"Number of clusters formed: {num_clusters}")

# Define marker shapes for each cluster
markers = ['o', 's', 'D', 'X']  # Circle, Square, Diamond, X
colors = ['r', 'g', 'b']  # Colors for true labels (0, 1, 2)

# Create a figure with two subplots
fig, axs = plt.subplots(1, 2, figsize=(14, 6))

# Plot clusters with different shapes
for cluster in range(num_clusters):
    cluster_points = iris_data_normalized[best_clusters == cluster]
    
    # Use the third and fourth features (Petal length and Petal width)
    axs[0].scatter(
        cluster_points[:, 2],  # Petal length
        cluster_points[:, 3],  # Petal width
        marker=markers[cluster % len(markers)],  # Different shape for each cluster
        label=f'Cluster {cluster}'
    )

axs[0].set_title('Clusters Visualization')
axs[0].set_xlabel('Petal Length (standardized)')
axs[0].set_ylabel('Petal Width (standardized)')
axs[0].legend()
axs[0].grid()

# Plot true labels with different colors
for label in np.unique(true_labels):
    label_points = iris_data_normalized[true_labels == label]
    
    axs[1].scatter(
        label_points[:, 2],  # Petal length
        label_points[:, 3],  # Petal width
        label=f'Class {label}',
        alpha=0.5  # Slight transparency for better visualization
    )

axs[1].set_title('True Labels Visualization')
axs[1].set_xlabel('Petal Length (standardized)')
axs[1].set_ylabel('Petal Width (standardized)')
axs[1].legend()
axs[1].grid()

# Set the main title for the entire figure
plt.suptitle("Single Linkage Optimal Distance Threshold")

plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to reserve space for the main title
plt.show()

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>Part 2: K-means Clustering</h2>
</div>
<div class="markdown-cell">
<h3>Exercise 2.1: Perform K-means clustering on Iris dataset</h3>
<p>In this exercise, you will apply K-means clustering to the Iris dataset and analyze how the number of clusters affects the clustering accuracy. Follow these steps:</p>
<ol>
<li>Implement K-means clustering for a range of cluster numbers (from 1 to 10).</li>
<li>Evaluate the clustering accuracy for each number of clusters.</li>
<li>Visualize the results to determine the optimal number of clusters.</li>
</ol>
<h4>Instructions:</h4>
<ol>
<li>Create a loop that iterates through the range of cluster numbers (1 to 10).</li>
<li>For each iteration:
   a. Perform K-means clustering using <code>sklearn.cluster.KMeans</code>.
   b. Use the <code>fit_predict</code> method to obtain cluster assignments.
   c. Store the cluster assignments in a list called <code>kmeans_results</code>.</li>
<li>Calculate the accuracy for each clustering result.</li>
<li>Generate a plot showing:
   a. The number of clusters vs. clustering accuracy.
   b. A scatter plot of the Iris data using Petal Length and Petal Width, colored by:<ul>
<li>The cluster assignments for the optimal number of clusters.</li>
<li>The true class labels.</li>
</ul>
</li>
</ol>
<p>This exercise will help you understand how the choice of cluster number affects K-means performance and how well the algorithm can recover the natural classes in the Iris dataset.</p>
</div>
<div class="markdown-cell">
<p>To facilitae this task we created the accuracy and visualization functions for you!</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Perform K-means

# return kmeans_results
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>run the following code for accuary and visulaization</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Calculate accuracies separately
accuracies = []
for clusters in kmeans_results:
    # Using sklearn's confusion_matrix to generate the contingency matrix
    contingency_matrix = confusion_matrix(true_labels, clusters)

    # For each cluster, find the maximum true label count
    correct_predictions = sum(np.max(contingency_matrix, axis=0))

    # Calculate accuracy
    accuracy = correct_predictions / len(true_labels)
    accuracies.append(accuracy)

# Plot setup
fig, axs = plt.subplots(1, 3, figsize=(24, 6))

# Plotting number of clusters vs. accuracy on the left subplot
axs[0].plot(cluster_range, accuracies, marker='o', color='b', label="Accuracy")
axs[0].set_title("Number of Clusters vs. Accuracy")
axs[0].set_xlabel("Number of Clusters")
axs[0].set_ylabel("Accuracy")
axs[0].set_xlim(0, 11)  # Setting x-axis limit
axs[0].set_ylim(0, 1.1)  # Setting y-axis limit
axs[0].set_xticks(range(0, 12))  # Setting x-ticks every 1 increment
axs[0].set_yticks(np.arange(0, 1.1, 0.1))  # Setting y-ticks every 0.1 increment
axs[0].grid()
axs[0].legend()

# K-means clustering with 3 clusters for visualization on the middle subplot
kmeans_3 = KMeans(n_clusters=3, random_state=0)
clusters_3 = kmeans_3.fit_predict(iris_data_normalized)

# Plot petal length vs. petal width with round markers for clusters
colors = ['r', 'g', 'b']  # Define colors for each cluster
for cluster in np.unique(clusters_3):
    cluster_points = iris_data_normalized[clusters_3 == cluster]
    axs[1].scatter(cluster_points[:, 2], cluster_points[:, 3],
                   color=colors[cluster],
                   label=f'Cluster {cluster}',
                   alpha=0.7)

# Customizing the plot for petal length and width
axs[1].set_title("K-means Clustering (3 Clusters) - Petal Length vs. Petal Width")
axs[1].set_xlabel("Petal Length")
axs[1].set_ylabel("Petal Width")
axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
axs[1].grid()

# K-means clustering with class coloring on the right subplot
for label in np.unique(true_labels):
    label_points = iris_data_normalized[true_labels == label]
    axs[2].scatter(label_points[:, 2], label_points[:, 3],
                   color=colors[label],
                   label=f'Class {label}',
                   alpha=0.5)

# Customizing the plot for petal length and width with class colors
axs[2].set_title("True Labels - Petal Length vs. Petal Width")
axs[2].set_xlabel("Petal Length")
axs[2].set_ylabel("Petal Width")
axs[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
axs[2].grid()

plt.tight_layout()
plt.show()

# Print final clustering accuracy
print("Clustering Accuracies:", accuracies)

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>Part 3: Fuzzy C-Means Clustering</h2>
</div>
<div class="markdown-cell">
<h3>Exercise 3.1: Perform C-means clustering on Iris dataset</h3>
<p>In this exercise you will perform C-means clustering on Iris dataset, test with up to 10 clusters and investigate how your choices affect accuracy. Generate a plot visualizing the optimal number of clusters using Petal Length and Width.</p>
<h4>Instructions:</h4>
<ol>
<li>
<p>Create a loop that iterates through a range of cluster numbers from 1 to 10, cluster_range = range(1, 11).</p>
</li>
<li>
<p>For each number of clusters:</p>
</li>
</ol>
<p>a. Perform Fuzzy C-Means clustering using the <code>fuzz.cluster.cmeans()</code> function from the skfuzzy library.</p>
<p>b. Use the normalized Iris data (<code>iris_data_normalized.T</code>) as input.</p>
<p>c. Set the fuzziness coefficient (m) to 2.</p>
<p>d. Use an error threshold of 0.005 and a maximum of 1000 iterations.</p>
<p>e. Extract the cluster centers and membership matrix from the FCM results.</p>
<p>f. Assign each data point to the cluster with the highest membership value.</p>
<p>g. Store the cluster assignments in a list called <code>all_clusters</code>.</p>
<ol>
<li>
<p>Your function should return:</p>
</li>
<li>
<p><code>all_clusters</code>: A list of numpy arrays, where each array contains the cluster assignments for a specific number of clusters.</p>
</li>
</ol>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Perform Fuzzy C means

# return all_clusters 
</code></pre>
</div>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">

# Calculate accuracies for each clustering result
for clusters in all_clusters:
    # Using sklearn's confusion_matrix to generate the contingency matrix
    contingency_matrix = confusion_matrix(true_labels, clusters)
    
    # For each cluster, find the maximum true label count
    correct_predictions = sum(np.max(contingency_matrix, axis=0))

    # Calculate accuracy
    accuracy = correct_predictions / len(true_labels)
    accuracies.append(accuracy)

# Plot setup
fig, axs = plt.subplots(1, 3, figsize=(24, 6))

# Plotting number of clusters vs. accuracy on the left subplot
axs[0].plot(cluster_range, accuracies, marker='o', color='b', label="Accuracy")
axs[0].set_title("Number of Clusters vs. Accuracy")
axs[0].set_xlabel("Number of Clusters")
axs[0].set_ylabel("Accuracy")
axs[0].set_xlim(0, 11)  # Setting x-axis limit
axs[0].set_ylim(0, 1.1)  # Setting y-axis limit
axs[0].set_xticks(range(0, 12))  # Setting x-ticks every 1 increment
axs[0].set_yticks(np.arange(0, 1.1, 0.1))  # Setting y-ticks every 0.1 increment
axs[0].grid()
axs[0].legend()

# Fuzzy C-means clustering with 3 clusters for visualization on the middle subplot
n_clusters_3 = 3
cntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(iris_data_normalized.T, n_clusters_3, 2, error=0.005, maxiter=1000)
clusters_3 = np.argmax(u, axis=0)

# Plot petal length vs. petal width with round markers for clusters
colors = ['r', 'g', 'b']  # Define colors for each cluster
for cluster in np.unique(clusters_3):
    cluster_points = iris_data_normalized[clusters_3 == cluster]
    axs[1].scatter(cluster_points[:, 2], cluster_points[:, 3],
                   color=colors[cluster],
                   label=f'Cluster {cluster}',
                   alpha=0.7)

# Customizing the plot for petal length and width
axs[1].set_title("Fuzzy C-means Clustering (3 Clusters) - Petal Length vs. Petal Width")
axs[1].set_xlabel("Petal Length")
axs[1].set_ylabel("Petal Width")
axs[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
axs[1].grid()

# Fuzzy C-means clustering with class coloring on the right subplot
for label in np.unique(true_labels):
    label_points = iris_data_normalized[true_labels == label]
    axs[2].scatter(label_points[:, 2], label_points[:, 3],
                   color=colors[label],
                   label=f'Class {label}',
                   alpha=0.5)

# Customizing the plot for petal length and width with class colors
axs[2].set_title("True Labels - Petal Length vs. Petal Width")
axs[2].set_xlabel("Petal Length")
axs[2].set_ylabel("Petal Width")
axs[2].legend(loc='upper left', bbox_to_anchor=(1, 1))
axs[2].grid()

plt.tight_layout()
plt.show()

# Print final clustering accuracy
print("Clustering Accuracies:", accuracies)

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>Exercise 3.2: Visualize membership matrix from toy datset</h3>
<p>In this exercise, you will create a single-variable dataset named <code>data_points</code> consisting of 200 equally spaced data points within the range of 0 to 10. After generating this dataset, you will apply the Fuzzy C-Means (FCM) algorithm with 5 clusters to analyze the data. Finally, you will visualize the data points against the membership values retrieved from the FCM algorithm, allowing you to observe how each data point is associated with the different clusters.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Step 1: Create a single variable dataset with 200 datapoints from 0 to 10

# return cluster centers cntr 
# return membership matrix u

</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>use the following code to visualize the memberships</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Step 3: Plotting the membership values
plt.figure(figsize=(12, 6))

# Plot membership values for each cluster
for i in range(n_clusters):
    plt.plot(data_points.flatten(), u[i], label=f'Cluster {i + 1}')

plt.title("Membership Values from Fuzzy C-Means Clustering")
plt.xlabel("Data Points")
plt.ylabel("Membership Value")
plt.xlim(0, 10)
plt.ylim(0, 1)
plt.grid()
plt.legend()
plt.show()

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>Exercise 3.3: Estimating membership values</h3>
<p>In addition to your existing data, add 200 points each from (-10) to (0) and (10) to (20) to create a larger dataset named extended_data_points, giving a total of 600 points spanning (-10) to (20).</p>
<ol>
<li>
<p><strong>Calculate Membership Values</strong>:<br />
   For the full range of data points, use the following formula to calculate the membership values for each cluster:
   $$
   \mu_k^{(n)} = \frac{1}{\sum_{j=1}^C \left(\frac{|x^{(n)} - v^{(k)}|}{|x^{(n)} - v^{(j)}|}\right)^{\frac{2}{m-1}}}
   $$
   where (m = 2) and (C = 5).</p>
</li>
<li>
<p><strong>Plot the Membership Values</strong>:<br />
   Visualize the membership values across the entire range. What do you observe?</p>
</li>
</ol>
<p>To achieve this task you can either use the provided formula and your previously obtained cluster centers cntr, or you can use the fuzz.cluster.cmeans_predict function.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">


# Step 1: Extend the data range from -10 to 20

# Step 2: Calculate membership values for the extended data using the obtained cluster centers
# Note: `cntr` is obtained from the previous code



</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>use the following code to visualize your membership values</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Step 3: Plot the membership values for the extended data range
plt.figure(figsize=(12, 6))

# Plot membership values for each cluster
for i in range(n_clusters):
    plt.plot(extended_data_points.flatten(), u_extended[i], label=f'Cluster {i + 1}')

plt.title("Membership Values for Extended Data Range from Fuzzy C-Means Clustering")
plt.xlabel("Data Points")
plt.ylabel("Membership Value")
plt.xlim(-10, 20)
plt.ylim(0, 1)
plt.grid()
plt.legend()
plt.show()
</code></pre>
</div>
</div>
<div class="markdown-cell">
<h3>Exercise 3.4: Investigating Clustering Quality</h3>
<p>In this exercise, you will employ the Xie-Beni index to compare the clustering quality of Fuzzy C-means across different numbers of clusters. Then, you will use a fixed number of clusters and explore the effect of the fuzziness parameter $m$ on the clustering quality. You will use the Iris dataset for this exercise.</p>
<p>The formula for the Xie-Beni Index is: </p>
<ol>
<li>
<p><strong>Numerator ( Cluster Compactness)</strong></p>
</li>
<li>
<p><strong>Denominator (Clusters Separation)</strong></p>
</li>
</ol>
<h3>Complete Xie-Beni Index Formula</h3>
<p>Putting it all together, the complete formula for the Xie-Beni Index is:</p>
<p>$$
\text{Xie-Beni Index} = \frac{\sum_{i=1}^{k} \sum_{j=1}^{N} u_{ij}^m \cdot |x_j - c_i|^2}{N \cdot \min_{i \neq j} |c_i - c_j|^2}
$$
(ref https://doi.org/10.1109/ICoSNIKOM48755.2019.9111538)</p>
<h3>Explanation of Variables:</h3>
<ul>
<li><strong>$N$</strong>: Total number of data points.</li>
<li><strong>$k$</strong>: Total number of clusters.</li>
<li><strong>$u_{ij}$</strong>: Membership value of data point ( j ) in cluster ( i ).</li>
<li><strong>$m$</strong>: Fuzziness parameter, which affects the degree of membership.</li>
<li><strong>$x_j$</strong>: The data point ( j ).</li>
<li><strong>$c_i$</strong>: The center of cluster ( i ).</li>
<li><strong>$|x_j - c_i|$</strong>: The Euclidean distance between data point ( j ) and cluster center ( i ).</li>
<li><strong>$\min_{i \neq j} |c_i - c_j|^2$</strong>: The minimum squared distance between any two distinct cluster centers $c_i$ and $c_j$.</li>
</ul>
<h4>detailed Instruction:</h4>
<ol>
<li>Compute Xie-Beni Index for varying number of clusters:</li>
<li>Iterate through a range of cluster numbers (2 to 10)</li>
<li>Perform Fuzzy C-Means clustering for each number of clusters</li>
<li>Calculate the Xie-Beni Index for each clustering result</li>
<li>
<p>Store the Xie-Beni Index values</p>
</li>
<li>
<p>Plot Xie-Beni Index vs. Number of Clusters:</p>
</li>
<li>Create a subplot</li>
<li>Plot the Xie-Beni Index values against the number of clusters</li>
<li>
<p>Set appropriate labels, title, and grid</p>
</li>
<li>
<p>Compute Xie-Beni Index for varying fuzzification index (m):</p>
</li>
<li>Fix the number of clusters to 3</li>
<li>Iterate through a range of m values (2 to 10)</li>
<li>Perform Fuzzy C-Means clustering for each m value</li>
<li>Calculate the Xie-Beni Index for each clustering result</li>
<li>
<p>Store the Xie-Beni Index values</p>
</li>
<li>
<p>Plot Xie-Beni Index vs. Fuzzification Index:</p>
</li>
<li>Create another subplot</li>
<li>Plot the Xie-Beni Index values against the fuzzification index values</li>
<li>
<p>Set appropriate labels, title, and grid</p>
</li>
<li>
<p>Display the plots:</p>
</li>
<li>Adjust the layout of the subplots</li>
<li>Show the final figure with both plots</li>
</ol>
<p>These tasks collectively analyze the performance of Fuzzy C-Means clustering on the Iris dataset using the Xie-Beni Index as a metric, exploring how it changes with different numbers of clusters and fuzzification index values.</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Write your code here for Xie-Beni Index Exercise 


# Show the Xie-Beni Index plots

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>---</h2>
</div>
<div class="markdown-cell">
<h2>Part 4: Gustafson-Kessel Clustering</h2>
</div>
<div class="markdown-cell">
<h3>Exercise 3.1: Perform GK Clustering on Iris dataset</h3>
<p>In this exercise you will perform  GK clustering on Iris dataset, test with up to 10 clusters and investigate how your choices affect accuracy. Generate a plot visualizing the optimal number of clusters using Sepal Length and Width.
For that, you are provided with the code for GK clustering algorithm, you need to fill in the missing parts in the code.</p>
</div>
<div class="markdown-cell">
<h4>Instructions:</h4>
<p>For each number of clusters, perform Gustafson-Kessel clustering on the iris_data_normalized. you will to initialize the cluster_range, partition_matrices, cluster_centers_list
* You should return:
   1. cluster_centers: The coordinates of the cluster centers.
   2. partition_matrix: The membership matrix showing the degree of belonging of each data point to each cluster</p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
# Initialize lists to store results


# return partition_matrices
# return cluster_centers
</code></pre>
</div>
</div>
<div class="markdown-cell">
<p><code>run the following code to estimate the accuracy and visualize your clusters</code></p>
</div>
<div class="code-cell">
<div class="input-area">
<pre><code class="language-python">
accuracies = []

# Second loop: Calculate accuracies
for partition_matrix in partition_matrices:
    # Assign each data point to the cluster with the highest membership value
    assigned_clusters = np.argmax(partition_matrix, axis=1)
    
    # Generate the contingency matrix
    contingency_matrix = confusion_matrix(true_labels, assigned_clusters)
    
    # Calculate accuracy by finding the most common true label in each cluster
    correct_predictions = np.sum(np.max(contingency_matrix, axis=0))
    accuracy = correct_predictions / len(true_labels)
    
    # Store the accuracy for the current number of clusters
    accuracies.append(accuracy)

# Plotting the results
plt.figure(figsize=(18, 6))

# Subplot 1: Number of Clusters vs Accuracy
plt.subplot(1, 3, 1)
plt.plot(cluster_range, accuracies, marker='o', linestyle='-', color='b')
plt.title('Clustering Accuracy vs. Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Accuracy')
plt.ylim(0, 1.1)  # Set y-axis range
plt.yticks(np.arange(0, 1.1, 0.1))
plt.grid()

# Subplot 2: Petal Length vs Petal Width for 3 Clusters (Clustered by GK)
assigned_clusters_3 = np.argmax(partition_matrices[1], axis=1)  # Index 1 corresponds to 3 clusters
cluster_centers_3 = cluster_centers_list[1]

# Define colors for clusters
colors = ['purple', 'green', 'orange']
plt.subplot(1, 3, 2)
for cluster in range(3):
    plt.scatter(iris_data_normalized[assigned_clusters_3 == cluster, 2],
                iris_data_normalized[assigned_clusters_3 == cluster, 3],
                color=colors[cluster], label=f'Cluster {cluster + 1}', edgecolor='k')
plt.scatter(cluster_centers_3[:, 2], cluster_centers_3[:, 3], marker='X', color='red', s=200, label='Cluster Centers')
plt.title('Petal Length vs Petal Width for 3 Clusters')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')
plt.legend()
plt.grid()

# Subplot 3: Petal Length vs Petal Width with True Class Coloring
true_colors = ['blue', 'red', 'yellow']  # Colors for true labels
plt.subplot(1, 3, 3)
for label in np.unique(true_labels):
    plt.scatter(iris_data_normalized[true_labels == label, 2],
                iris_data_normalized[true_labels == label, 3],
                color=true_colors[label], label=f'True Class {label}', edgecolor='k', alpha=0.5)
plt.title('Petal Length vs Petal Width - True Class')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

</code></pre>
</div>
</div>
<div class="markdown-cell">
<h2>---</h2>
</div>
<div class="markdown-cell">
<h1>End of instructions 4</h1>
</div>
<div class="markdown-cell">
<h2>---</h2>
<hr />
</div>
                </div>
            </div>
        </div>
    </div>

    <footer>
        <p id="footerYear"></p>
    </footer>

    <script src="../../../../../../script.js"></script>

    <script>
        // Note navigation functionality
        document.addEventListener('DOMContentLoaded', function() {
            loadNoteNavigation();
        });
        
        async function loadNoteNavigation() {
            try {
                // Get current page path relative to root
                const currentPath = window.location.pathname;
                const pathParts = currentPath.split('/');
                const fileName = pathParts[pathParts.length - 1];
                
                // Construct the relative path to match navigation data
                let relativePath = '';
                if (pathParts.includes('notes')) {
                    const notesIndex = pathParts.indexOf('notes');
                    relativePath = pathParts.slice(notesIndex).join('/');
                } else {
                    relativePath = 'notes/' + fileName;
                }
                
                // Decode URL encoding (e.g., %20 -> space) to match navigation data
                relativePath = decodeURIComponent(relativePath);
                
                // Load navigation data
                const response = await fetch('../../../../../../notes_navigation.json');
                if (!response.ok) {
                    console.log('Navigation data not found');
                    return;
                }
                
                const navigationData = await response.json();
                const currentNav = navigationData[relativePath];
                
                if (!currentNav) {
                    console.log('Current page not found in navigation data:', relativePath);
                    return;
                }
                
                // Update progress
                const progressText = document.getElementById('progressText');
                const progressFill = document.getElementById('progressFill');
                const progress = ((currentNav.current_index + 1) / currentNav.total_notes) * 100;
                
                progressText.textContent = `${currentNav.current_index + 1} of ${currentNav.total_notes}`;
                progressFill.style.width = `${progress}%`;
                
                // Update current page title in header
                const currentPageTitle = document.getElementById('currentPageTitle');
                const pageTitle = relativePath.split('/').pop().replace('.html', '').replace(/_/g, ' ');
                currentPageTitle.textContent = pageTitle;
                
                // Update previous button
                const prevButton = document.getElementById('prevButton');
                
                if (currentNav.previous) {
                    prevButton.href = '../../../../../../' + currentNav.previous;
                    prevButton.title = 'Previous: ' + currentNav.previous_title;
                    prevButton.style.visibility = 'visible';
                } else {
                    prevButton.style.visibility = 'hidden';
                }
                
                // Update next button
                const nextButton = document.getElementById('nextButton');
                
                if (currentNav.next) {
                    nextButton.href = '../../../../../../' + currentNav.next;
                    nextButton.title = 'Next: ' + currentNav.next_title;
                    nextButton.style.visibility = 'visible';
                } else {
                    nextButton.style.visibility = 'hidden';
                }
                
                // Add keyboard navigation
                document.addEventListener('keydown', function(e) {
                    if (e.ctrlKey || e.metaKey) return; // Don't interfere with browser shortcuts
                    
                    if (e.key === 'ArrowLeft' && currentNav.previous) {
                        window.location.href = '../../../../../../' + currentNav.previous;
                    } else if (e.key === 'ArrowRight' && currentNav.next) {
                        window.location.href = '../../../../../../' + currentNav.next;
                    }
                });
                
            } catch (error) {
                console.error('Error loading navigation:', error);
                document.getElementById('progressText').textContent = 'Navigation unavailable';
            }
        }
    </script>

</body>
</html>