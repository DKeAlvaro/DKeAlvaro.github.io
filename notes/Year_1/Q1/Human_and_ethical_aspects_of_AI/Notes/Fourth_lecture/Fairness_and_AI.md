# Human and ethical aspects of AI - Lecture 4

Q: What are the four approaches mentioned for incorporating ethics into AI applications?
A: 1. Build the system using ethical rules/values; 2. Train systems on data where rules are followed; 3. Train on data where rules *might not* have been followed, but use rules as filters; 4. Rely on the socio-technical system (users) to provide ethical input (Page 5).

Q: What are some limitations of the "Design for Values" (DfV) approach?
A: Some ethical issues appear too late in the design process; DfV is very resource-intensive; and it may be impossible to specify values in cases of high moral uncertainty ('moral disruption') (Page 7).

Q: What is the guiding assumption of "just war theory"?
A: The only justification for war is self-defense (Page 9).

Q: What are the two main branches of "just war theory"?
A: *Jus ad bellum* (justice *of* war) and *jus in bello* (justice *in* war) (Page 9).

Q: What are two key principles of *jus in bello* (justice *in* war)?
A: The principles of proportionality and discrimination (Page 9).

Q: According to Cummings (2006), what is the primary purpose of analyzing ethical issues in a military system?
A: To identify human value design considerations that will guide the engineer in constructing a more effective decision support system (Page 10).

Q: What was the key limitation of the original Tomahawk missile?
A: It could not be redirected in-flight, leading to potential redundant demolition of targets or the inability to correct a targeting mistake (Page 11).

Q: What is "automation bias"?
A: The tendency for human operators to accept the default of automated, "smart" systems, even if they have other information that should lead them to double-check it (Page 14).

Q: What is the core argument for why algorithmic fairness must represent well-being or need?
A: For fairness to be non-accidental, its elements (like prior well-being, need, and group parity) must be represented in the algorithm (Page 16).

Q: What is the "Parity-based approach" to algorithmic fairness?
A: An approach that aims to achieve a form of parity of outcomes between different groups (Page 18).

Q: What is a "worry" or objection to the idea that fairness must be actively represented in an algorithm?
A: If fairness is conceptualized as "non-bias," then we only need to ensure no bias exists, not actively represent the elements of fairness (Page 21).

Q: What is "selection bias" in an algorithm?
A: The training data does not represent the population to which a model is applied (Page 22).

Q: What is "model bias" in an algorithm?
A: The model itself introduces incorrect assumptions (e.g., in what it takes as input for a risk score) (Page 22).

Q: What problem does the lecture highlight regarding the classification of "ethnicity"?
A: The technical assumptions of a classification system demand mutually exclusive categories, which is problematic for a concept like race and may not capture reality (Page 23).

Q: What are the problems with using "fairness as algorithmic non-bias" (i.e., a "blind" model)?
A: Background inequalities are left in place, and "proxies" for a characteristic (like race) might be included accidentally or intentionally (Page 24).

Q: What is the difference between "procedural fairness" and "substantive fairness"?
A: Procedural concepts describe *how* decisions should be made (the process), while substantive notions refer to the *outcome* or goal the decision should achieve (Page 25).

Q: What is the difference between "Contractualism" and "Contractarianism"?
A: Contractualism (Hobbes) is what people *actually* agree to, while Contractarianism (Rawls) is what *reasonable* people *would* agree to under ideal circumstances (Page 26).

Q: What is the "Veil of Ignorance" in Rawlsian justice?
A: A hypothetical condition where people agree on principles of justice *without* knowing which position in society they will go on to occupy (Page 27).

Q: What is the "Difference Principle"?
A: Inequalities are permitted only when they benefit those who are least well off (Page 28).

Q: What is "Prioritarianism"?
A: Increases in well-being for those who are less well-off count more than similar increases for those who are better off (Page 28).

Q: What is the "Rawlsian rule for algorithmic fairness" proposed by Barsotti & Kocer (2024)?
A: 'A model is fair if it does not make more systematic errors for any [relevant] sub-group in the dataset compared to the others' (Page 29).

Q: What is the main argument from Winters et al. (2020) regarding digital health and justice?
A: Based on Prioritarianism, we should favor contextual and targeted initiatives because they benefit the poor more, even if they require more resources than general, scalable tools (Page 33).

Q: What is "relational justice"?
A: An approach to justice that focuses on power and history instead of a general principle of distribution (Page 34).

Q: What slogan is associated with relational justice in data ethics?
A: "Nothing about us, without us" (Page 34).

Q: What was the Havasupai tribe case about?
A: Their DNA samples, collected for diabetes studies, had been used without consent for other genetic studies (Page 35).

Q: What is "punitive justice"?
A: Justice related to decisions about jail or sentencing, guided by the principle that "justice is blind" to arbitrary characteristics (Page 36).

Q: What is the argument against using statistical tools like the Public Safety Assessment (PSA) for detention?
A: We should not deny people their liberty on statistical grounds; individualized evidence should be required (Page 37).

Q: What advice does the lecture give to AI experts who want to be useful to the world?
A: To "dive into" the domain (e.g., study the history of segregation if you care about housing) and not be "domain agnostic" (Page 38).

Overview: Fourth lecture

Date: 11 Sep 2025