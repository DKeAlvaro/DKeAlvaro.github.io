# Superintelligence and risks of AI - Lecture 10

Q: According to Goodall, what are the three phases in programming moral machines?
A: 1. Rational (rule-based or outcome-based), 2. AI-based (using training data), and 3. Natural language feedback (XAI). (Page 3)

Q: What are Goodall's critiques of the "Rational" phase approaches?
A: Rule-based ethics is too minimal for unclassified cases, while outcome-based ethics recommends some morally bad decisions. (Page 3)

Q: What are the problems with the "AI-based" phase (using training data) according to Goodall?
A: It would reproduce human shortcomings and lack explainability. (Page 3)

Q: According to Awad et al., why is ethics preferred over direct regulation for AI?
A: Black Box ML makes principles opaque, technology changes faster than regulation, and the source of errors is hard to trace. (Page 5)

Q: What is "SITL" and why do Awad et al. argue for it?
A: "Society in the loop"; it's considered necessary for a dynamic consensus on AI ethics, which is needed to adopt the benefits of automation. (Page 6)

Q: What general preferences did the Moral Machines project identify?
A: People prefer sparing the lawful (over unlawful), humans (over pets), the greater number, and younger humans (over older). (Page 7)

Q: What is the superficial definition of risk?
A: risk = probability $\times$ consequence. (Page 12)

Q: What is the difference between risk, uncertainty, and ignorance?
A: Risk has a known probability, uncertainty means we cannot attribute a probability, and ignorance means we don't even know the scenarios ('we don't know what we don't know'). (Page 13)

Q: What is the general problem with AI bias found in training data?
A: The data reflects existing underlying inequalities in society. (Page 16)

Q: What is a "dual-use technology"?
A: A technology that can be used for both civil and military purposes. (Page 17)

Q: How does AI enhance the threat of disinformation?
A: AI reduces the cost of generating mis/disinformation at scale. (Page 19)

Q: How is the job displacement risk from AI different this time?
A: AI targets cognitive tasks (like coding, creativity, and decision-making), not just manual labor. (Page 20)

Q: What is the "enfeeblement and de-skilling" risk described by Stuart Russell?
A: If everyone stops learning skills that machines already have, the human race will collectively lose its autonomy. (Page 22)

Q: What is the "Singularity"?
A: A theoretical point where artificial intelligence surpasses human intelligence, becomes uncontrollable, and leads to societal change. (Page 23)

Q: What is "Superintelligence" as defined by Bostrom?
A: "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest". (Page 24)

Q: What is the "Orthogonality Thesis"?
A: The idea that intelligence and final goals are separate (orthogonal), meaning any level of intelligence can be combined with any final goal (e.g., paperclip-maximizing). (Page 25)

Q: What is the "Instrumental Convergence Thesis"?
A: The idea that intelligent agents with varied final goals will still pursue similar intermediate goals, such as self-preservation and acquiring resources. (Page 26)

Q: What is Chalmers' 3-step argument for the arrival of $AI++$ (superintelligence)?
A: 1. There will soon be AI. 2. If there is AI, there will be $AI+$ (human-level) soon after. 3. If there is $AI+$, there will be $AI++$ soon after. (Page 30)

Q: What is "Value alignment"?
A: The idea that AI's values will align with human values. (Page 31)

Q: What is "Long-termism"?
A: The idea that the existence of sentient beings in the distant future should strongly influence our decisions now. (Page 31)

Q: What is an "Existential Risk"?
A: A risk that threatens the extinction of humanity, civilizational collapse, or the permanent loss of our potential for desirable future development. (Page 32)

Q: What is a "catastrophic risk"?
A: A non-zero probability of a catastrophic outcome, usually involving harm on a large scale like a pandemic, nuclear war, or climate change. (Page 33)

Q: What is a Type-I error in risk assessment?
A: A mistake of assuming a statement is true while it is false (a false positive). (Page 35)

Q: What is a Type-II error in risk assessment?
A: A mistake of assuming a statement is false while it is true (a false negative). (Page 35)

Q: What are four key considerations in the "ethics of risk" (i.e., when is it permissible to impose risks)?
A: Do benefits outweigh costs? Are risks fairly distributed? Would people agree to the risks? What are the alternatives? (Page 37)

Q: According to McKinsey (2023), which job sectors will see the largest *decrease* in labor demand by 2030 due to AI?
A: Office support (–18%) and Customer service and sales (–13%). (Page 38)

Q: According to McKinsey (2023), which job sectors will see the largest *increase* in labor demand?
A: Health professionals (+30%), Health aides/technicians (+30%), and STEM professionals (+23%). (Page 38)

Q: What is "Prioritarianism"?
A: A principle of justice that gives priority to the worse off over the better off. (Page 39)

Q: What are the four risk categories in the EU AI Act?
A: Unacceptable Risk, High Risk, Limited Risk, and Minimal Risk. (Page 40)

Q: What is "deep uncertainty"?
A: A situation where it is impossible to predict a full range of realistic possible outcomes. (Page 43)

Q: What is the "Precautionary Principle" (from the Rio Declaration)?
A: Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures. (Page 42, 44)

Q: What did the "Pause Giant AI Experiments" open letter call for?
A: A pause of at least 6 months on the training of AI systems more powerful than GPT-4. (Page 45)

Overview: Tenth lecture
Date: 10 Oct 2025