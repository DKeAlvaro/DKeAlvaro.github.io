# Design for Values and AI - Lecture 3

Q: What is "Technical AI"?
A: A set of technical methods for simulating learning and reasoning in machines (Page 3).

Q: How is "Reasoning" defined in Technical AI?
A: It is deductive or inductive and follows the rules of logic (Page 3).

Q: What is "Robustness" in Technical AI?
A: The reliability of a model for new data (Page 3).

Q: What does the EU approach to "trustworthy AI" incorporate?
A: Ethics and robustness (Page 3).

Q: What is the aim of "Classical AI"?
A: To build artificial machines that have intelligence (Page 4).

Q: What is the claim of "Strong AI"?
A: That an AI *actually has* a mind and cognition (Page 4).

Q: What is "Artificial General Intelligence" (AGI)?
A: The ambition to recreate human or superhuman intelligence using the current methodology of Technical AI (Page 5).

Q: What are two main philosophical arguments that intelligence is "computation-plus"?
A: The 4E argument (intelligence must be embodied) and the Chinese room argument (intelligence requires intentionality) (Page 7).

Q: What is the "Design for Values" (DfV) paradigm?
A: An approach to incorporating ethical values into a technological design (Page 9).

Q: Where does "Design for Values" get its input from?
A: Stakeholders and Ethical theories, especially mid-level ethical theories (Page 9).

Q: What are the "disruptive" features of AI that create special challenges for DfV?
A: Rapid adoption, massive scale, independent intelligent steps, and opacity (Page 10).

Q: What are three methodological assumptions of DfV?
A: 1. It is possible and necessary to build values in early; 2. Each technology can be embedded in different ways to create different socio-technical systems; 3. Our values can change over time (Page 11).

Q: What are the three aspects of Design for Values (CHLEP p. 72)?
A: 1. Identifying the relevant values, 2. Embedding these values in systems, 3. Assessing whether these efforts have been successful (Page 12).

Q: What are the three investigations in "Value Sensitive Design" (VSD)?
A: Conceptual, Technical, and Empirical (Page 14).

Q: What is the "Conceptual" investigation in VSD?
A: Identify relevant values and find possible design solutions (Page 14).

Q: What is the "Technical" investigation in VSD?
A: Assess value tradeoffs within designs and try out technical solutions (Page 14).

Q: What is the "Empirical" investigation in VSD?
A: Devise empirical value criteria and measure values in the implementation of designs (Page 14).

Q: What are the three "fundamental" ethical values mentioned?
A: Happiness/welfare, Autonomy/respect for autonomy, and Justice (Page 19).

Q: What are some different interpretations of the value "Justice"?
A: Everybody gets what they deserve, an equal amount, or what they need (Page 20).

Q: What are three important distinctions for discussing ethical values?
A: Descriptive vs. normative judgments, Non-ethical vs. ethical value, and Intrinsic vs. instrumental value (Page 21).

Q: What is the difference between a "descriptive" and "normative" judgment?
A: A descriptive judgment states a fact (e.g., "This building is square"), while a normative judgment states an evaluation (e.g., "This building is good") (Page 22).

Q: What is the difference between "intrinsic" and "instrumental" value?
A: Intrinsic value is the value something has "in itself," while instrumental value is its value as a tool or means to an end (Page 23).

Q: How does DfV differ from VSD, according to the lecture?
A: DfV has a greater emphasis on *theory* as a source of values (Page 24).

Q: What are "mid-level" ethical theories, and what are some examples?
A: Theories that provide specific guidance about values to include, such as Rawls' theory of justice or the Capability Approach (Page 24).

Q: What are the three cases discussed in CHLEP Ch. 3?
A: ChatGPT (and other LLMs), AI for determining loan eligibility, and AI for benefits fraud detection (Page 26).

Q: What are some limitations of Design for Values?
A: Ethical issues may appear too late, it's resource-intensive, it may be impossible in cases of 'moral disruption', and some technologies may be impossible to "ethically optimize" (Page 27).


Overview: Third lecture
Date: 9 Sep 2025