# Moral machines - Lecture 9

Q: What are the two main value paradigms for human-technology interaction discussed in the review?
A: Meaningful Human Control (MHC) and Human-Centered AI (HCAI) (Page 3).

Q: What ideal are MHC and HCAI contrasted with?
A: The Technological Autonomy Ideal (TAI) (Page 3).

Q: What is the "Technological Autonomy Ideal" (TAI) as a normative ideal?
A: We should strive for maximal technological autonomy in technology design (Page 5).

Q: What is the "Human Centered AI" (HCAI) ideal, according to Shneiderman (2020)?
A: We should strive for high automation together with high human control in technology design (Page 5).

Q: How does the lecture compare TAI and HCAI in terms of adaptation?
A: TAI is comparable to adapting human nature to AI, while HCAI is comparable to adapting AI and robots to human nature (Page 5).

Q: According to Shneiderman's 2x2 framework, what are the two axes?
A: Human Control (High/Low) and Computer Automation (High/Low) (Page 4).

Q: In Shneiderman's framework, what quadrant do "Pacemaker" and "Airbag" fall into?
A: Low Human Control and High Computer Automation, labeled "Computer Control" (Page 4).

Q: In Shneiderman's framework, what quadrant do "Bicycle" and "Piano" fall into?
A: High Human Control and Low Computer Automation, labeled "Human Mastery" (Page 4).

Q: In Shneiderman's framework, what is the ideal quadrant, labeled "Reliable, Safe & Trustworthy"?
A: High Human Control and High Computer Automation (Page 4).

Q: What is a key practical argument against TAI mentioned by Goldenfein et al. (2020)?
A: TAI will require remote operators to monitor and take back tasks, creating invisible, exploitative work (Page 6).

Q: What is a key practical problem with the HCAI ideal?
A: HCAI entails technology "handoffs," which raise questions about responsibility and liability (Page 6).

Q: What is a "handoff" in human-AI interaction?
A: A moment at which primary control transfers from one entity to another, such as from a human to automation or vice versa (Page 6, Page 22).

Q: According to Schneiderman, what three values does HCAI promote?
A: Reliability, Safety, and Trustworthiness (RST) (Page 7).

Q: What is the lecture's critique of Schneiderman's claim that HCAI promotes RST?
A: It is an empirical claim that needs a clear baseline of fully automated systems for comparison; isolated cases are not sufficient proof (Page 7).

Q: According to Schneiderman, what three characteristics of a system lead to human "Self-efficacy, mastery, and responsibility"?
A: Comprehensible, predictable, and controllable (Page 7).

Q: What apparent counterexample to HCAI is provided using the AlphaFold program?
A: It generates significant risks for scientists' ability to exercise their full capacities and utilize skills they invested years in developing (Page 8).

Q: What core value is the "Meaningful Human Control" (MHC) paradigm focused on achieving?
A: Responsibility (Page 10).

Q: What core value is the "Human-Centered AI" (HCAI) paradigm focused on achieving?
A: Trustworthiness (Page 10).

Q: What is the main conclusion of Sven Nyholm's (2020) argument?
A: We should adapt AI and robots to human nature, or we should adapt human nature to AI and robots (Page 12).

Q: What is an example of adapting AI to human nature in "mixed traffic"?
A: Make self-driving cars drive like humans (Page 18).

Q: What is an example of adapting human nature to AI in "mixed traffic"?
A: Seek means of making humans drive more like self-driving cars (Page 18).

Q: According to van de Poel & Royakkers, what are the 4 conditions on acceptable risk?
A: 1. Do the benefits outweigh the costs? 2. Availability of alternatives 3. Fair distribution of risks and benefits 4. Informed consent (Page 16).

Q: What is "task displacement" in automation?
A: When a mechanical system takes over a task from a human, like a laundry machine (Page 22).

Q: Why is it difficult to determine responsibility during handoffs?
A: Because it is difficult to determine who or what is in control, and responsibility depends on an attribution of control (Page 23).

Q: What is "Level 3 automation" in assisted driving, according to Goodall (2014)?
A: The driver is not required to remain attentive but must be available to take control within a certain amount of time after an alert (Page 21).

Q: What is "Machine ethics" as defined by Anderson and Anderson (2007)?
A: Ensuring that the behavior of machines toward human users, and perhaps other machines as well, is ethically acceptable (Page 25).

Q: What is the concept of "ALIGNMENT" in recent AI discussions?
A: AI reasoning should be able to take into account societal values, moral and ethical considerations (Page 25, Page 26).

Q: What is Asimov's First Law of Robotics?
A: A robot may not injure a human being or, through inaction, allow a human being to come to harm (Page 27).

Q: What was a key conclusion from Goodall (2014) regarding AI ethics?
A: "There was no obvious way to encode complex human morals effectively in software" (Page 28).

Q: What are the three phases Goodall (2014) proposed for developing ethical algorithms?
A: 1. Rational approach, 2. AI approach, 3. Natural language requirement (XAI) (Page 28).

Q: What did Goodall identify as the problem with a "Rational" (Phase 1) approach?
A: Rule-based ethics is too minimal, while outcome-based ethics recommends some morally bad decisions (Page 29).

Q: What did Goodall identify as the problem with an "AI-based" (Phase 2) approach?
A: It would reproduce human shortcomings and lack explainability (Page 29).

Q: According to Awad et al. (2020), why is it challenging to simply regulate AI?
A: The "Black Box" problem, technology changes faster than regulation, and the source of errors is hard to trace (Page 31).

Q: What is "SITL" as argued for by Awad et al. (2020)?
A: "Society in the loop," which is necessary for a dynamic consensus on the ethics of intelligent machines (Page 32).

Q: What are some of the global preferences identified by the "Moral Machines" project?
A: Sparing the lawful, sparing humans over pets, sparing the greater number, and sparing younger humans over older ones (Page 33).

Overview: Ninth lecture
Date: 10 Oct 2025