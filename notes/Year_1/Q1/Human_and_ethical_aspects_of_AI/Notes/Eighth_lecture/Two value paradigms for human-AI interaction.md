# Two value paradigms for human-AI interaction - Lecture 8

Q: What is a "responsibility gap"?
A: A concern that nobody can be reasonably held responsible for bad outcomes caused by a (semi-)autonomous system (Page 3).

Q: What are the three conditions an entity must satisfy to be held responsible?
A: Foreknowledge ("knew or should have known"), Control/causal influence, and Freedom ("could have done otherwise") (Page 3).

Q: What is the general definition of "Meaningful Human Control" (MHC) according to Santoni di Sio & van den Hoven (2018)?
A: "humans not computers and their algorithms should ultimately remain in control of, and thus morally responsible for, relevant decisions" (Page 4).

Q: According to Fischer & Ravizza, what are the two dimensions of control?
A: Guidance control and Regulative control (Page 7).

Q: What is "guidance control"?
A: It is based on reason-responsiveness and the attributability of an outcome to an agent (Page 7).

Q: What is "regulative control"?
A: It is based on the idea that the agent could have acted differently (freedom) (Page 7).

Q: Which dimension of control from Fischer & Ravizza is the basis for MHC?
A: Guidance control (Page 8).

Q: Which dimension of control from Fischer & Ravizza is *not* required for MHC?
A: Regulative control (Page 8, Page 9).

Q: What are the two aspects of Meaningful Human Control?
A: Tracking and Tracing (Page 8, Page 9).

Q: What is the "tracking" condition in MHC?
A: It is the "reason-responsiveness" aspect of guidance control (Page 8).

Q: What is the "tracing" condition in MHC?
A: It is the "each outcome is attributable to an agent" aspect of guidance control (Page 8).

Q: What are the four conditions for Nozick's "tracking" account of knowledge?
A: (i) P is true. (ii) S believes P. (iii) If P were not true, S would not believe P. (iv) If P were true, S would believe P (Page 10).

Q: What is the "tracing" condition based on from Fischer & Ravizza?
A: "Ownership" of an action or decision (Page 12).

Q: Why can a machine apparently not be responsible?
A: It does not satisfy the freedom condition, and there is a "retribution gap" (it doesn't make sense to blame or punish a machine) (Page 16).

Q: According to Santoni di Sio & van den Hoven, can humans be held responsible for actions delegated to smart machines?
A: Yes, "Delegation of activity to smart machines is compatible with genuine responsibility attributions" (Page 17).

Q: What is the main idea of MHC regarding responsibility?
A: When guidance control is satisfied by automation, the humans who built and operate the system can be held responsible (Page 17).

Q: What is the implication of the "tracing" condition for designers?
A: Designers must ensure human agents in the chain are "technically and psychologically capable" and "well aware of their responsibility" (Page 24).

Q: In the Tesla example where the driver doesn't swerve, is she responsible even if the autopilot would have blocked her?
A: MHC predicts yes, because she has guidance control (Page 25).

Q: In the reverse Tesla example where the autopilot doesn't swerve, are the designers responsible even if the human would have blocked it?
A: MHC predicts yes, because the autopilot has guidance control and the outcome is a foreseeable consequence of its design (Page 26).

Q: How does the lecture map the traditional conditions for responsibility to the MHC framework?
A: Foreknowledge -> guidance control, tracking; Control/causal influence -> guidance control, tracing; Freedom -> regulative control (Page 27).

Q: What is the "one-dimensional model" of automation?
A: A linear scale from "Complete human control" to "Autonomous mechanical control" (Page 29).

Q: What does the one-dimensional model of automation suggest?
A: A tradeoff between human control and automation (Page 30).

Q: What is the "two-dimensional model" of automation, according to Shneiderman (2020)?
A: A model with two axes: "More automation" (horizontal) and "More human control" (vertical) (Page 31).

Q: In Shneiderman's 2D framework, what is the ideal quadrant?
A: "Reliable, Safe & Trustworthy," characterized by High Human Control and High Computer Automation (Page 32).

Q: What is the "Technological Autonomy Ideal" (TAI)?
A: The normative ideal that we should strive for maximal technological autonomy in technology design (Page 33).

Q: What is the "Human Centered AI" (HCAI) ideal?
A: The normative ideal that we should strive for high automation together with high human control in technology design (Page 33).

Q: How do TAI and HCAI relate to human adaptation?
A: TAI implies adapting human nature to AI, while HCAI implies adapting AI and robots to human nature (Page 33).

Q: What is one of Shneiderman's critiques of the TAI?
A: "Humans have to spend more effort monitoring autonomous computers because they are unsure what it will do, often leading to inferior performance." (Page 34).

Q: What is another critique of TAI listed by Shneiderman?
A: "Computers are not teammates, collaborators, or co-active partners... Humans are responsible for actions of the technology assists that they use..." (Page 34).

Q: What is a key practical problem with TAI according to Goldenfein et al. (2020)?
A: It will require remote operators for monitoring, creating invisible and exploitative work (Page 35).

Q: What is a key practical problem with HCAI according to Goldenfein et al. (2020)?
A: It entails technology "handoffs" which create questions about responsibility and liability (Page 35).

Q: What is a "handoff" (or "handback")?
A: A moment at which primary control transfers from one entity to another (e.g., from human to automation or vice versa) (Page 36).

Q: What values does HCAI promote, according to Shneiderman?
A: Reliability, safety, and trustworthiness (Page 37).

Q: According to Shneiderman, what human values are supported by systems that are comprehensible, predictable, and controllable?
A: Self-efficacy, mastery, and responsibility (Page 37).

Q: What is the "apparent counterexample to HCAI" involving AlphaFold?
A: While it helps scientists, it also generates risks for their ability to exercise their full capacities and demonstrate mastery of skills they spent years developing (Page 38).

Q: What are the shared claims of MHC, HCAI, and related paradigms?
A: High automation can be combined with human contributions to achieve values like Responsibility (MHC) and Trustworthiness (HCAI) (Page 40).

Q: What is the relationship between trust and "discretionary authority" in professional ethics?
A: Trust is essential because it is the "willingness to make oneself vulnerable to the discretionary choices of another person" (Page 43).

Q: What is Baier's (2013) argument about transparency and trust?
A: Increasing monitoring and supervision (transparency) is an act of *decreasing* trust, not building it (Page 46).


Overview: Eighth lecture
Date: 25 Sep 2025