# Sustainability and AI - Lecture 6
Q: What is the concept of "Alignment" in AI-speak?
A: It is "Al-speak for 'Design for Values'" (Page 3).

Q: Why is it difficult to have AI systems with computational structures that directly correspond to social values like "fairness"?
A: Because of the conceptual complexities surrounding what "values" are (Page 4).

Q: What is a more realistic goal than embedding "values" into AI systems, according to the IEEE?
A: To embed explicit "norms" into such systems (Page 4).

Q: How can norms be expressed computationally?
A: In terms of obligations and prohibitions (Page 4).

Q: What is the "anthropocentric" view of sustainability?
A: "We have to protect nature for future generations," which sees an instrumental value in nature (Page 5).

Q: What is the "biocentric" view of sustainability?
A: "We have to protect nature for its own sake," which sees an intrinsic value in nature (Page 5).

Q: What is the "narrow conception of technology"?
A: The functional "stuff" under the control of engineers and designers (Page 7).

Q: What is the "broad conception of technology"?
A: The functional "stuff" plus all the users, institutions, and social elements needed to make it work in the real world (Page 7).

Q: What are "sociotechnical systems"?
A: Hybrid systems involving components from the material world as well as individual people, corporate actors (like businesses), and abstract social entities (like laws and institutions) (Page 8).

Q: What are some of the "far-flung elements" of an AI's sociotechnical system?
A: Data servers, mines for rare elements, and micro-workers (Page 11).

Q: What are "micro-workers" in the context of AI production?
A: Little-qualified, low-paid workers who annotate, tag, label, correct, and sort the data used to train and test AI solutions (Page 11).

Q: What are "negative externalities"?
A: Costs, risks, or harms created by an enterprise that do not appear on its balance sheet and are imposed on others (Page 12).

Q: What are some examples of negative externalities from hosting a large AI model?
A: Major disruptions to education, uncompensated pollution (like e-waste), and strain on the energy grid (Page 12).

Q: What normative premise can be formulated using the idea of negative externalities?
A: Enterprises should insure or compensate for their negative externalities (Page 12).

QD: What is the primary meaning of sustainability?
A: The acceptability of long-term environmental risks and harms (Page 13).

Q: What is "social sustainability"?
A: A more recent emphasis on sustainability regarding factors like employment and political stability (Page 13).

Q: What is the "Brundtland" definition of sustainability?
A: "development that meets the needs of the present without compromising the ability of future generations to meet their own needs" (Page 20).

Q: What is the "Precautionary Principle"?
A: A principle for dealing with uncertainty, stating that where there are threats to the environment or human health, precautionary measures should be taken even if cause-and-effect is not fully established (Page 22).

Q: What are the four dimensions of the Precautionary Principle?
A: Threat, uncertainty, action, and prescription (Page 22).

Q: What are the "two directions" of the sustainability dilemma in AI?
A: "AI for sustainability" (using AI to solve problems) and "Sustainability of AI" (addressing the impacts of AI itself) (Page 25).

Q: What is "data pollution" as defined by Hasselbalch & Van Wynsberghe?
A: It encompasses tangible and intangible impacts, including environmental damage, data "spills," privacy violations, and discrimination (Page 25).

Q: What are the three levels of "power dynamics" in sustainability ethics for AI?
A: Micro (an engineer's choices), Meso (company/institution standards), and Macro (the global system's trajectory) (Page 26).

Q: What is Lin's (2022) normative premise regarding climate change?
A: Limiting global warming to $1.5^{\circ}C$ and reaching zero emissions by mid-century are necessary steps (Page 28).

Q: What are the three policy proposals from Lin (2022) to address AI's carbon footprint?
A: 1. Monitoring emissions impact, 2. Climate-aware guidelines for AI training, 3. Clean energy for data centers (Page 29).

Q: What are the three points of impact of machine learning's "footprint" (Lin 2022)?
A: 1. The impact of training, 2. The impact of building/maintaining data centers, 3. The impact of real-life application (Page 32).

Q: What four factors can be used to estimate the environmental impact of LLMs?
A: 1. Training duration (computation), 2. Energy efficiency of GPUs, 3. Location of data center, 4. Offsets (if any) (Page 33).

Q: What was the key finding in the 2025 Kenyan court case regarding carbon offsets?
A: A flagship carbon offset project (used by Meta, Netflix, etc.) was halted for unconstitutionally and unlawfully acquiring community land without consent (Page 35).

Q: According to the IEEE, what should A/IS regulation, development, and deployment be based on?
A: International human rights standards and (in armed conflicts) international humanitarian laws (Page 36).

Q: What are the "Ruggie principles"?
A: The UN Guiding Principles for Business and Human Rights, which outline a business's obligation to respect international human rights (Page 36).

Overview: Sixth lecture
Date: 17 Sep 2025